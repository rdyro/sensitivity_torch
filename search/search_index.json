{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Optimization Sensitivity in PyTorch - Reference Documentation - First- and Second-order Optimization","text":"<p><code>sensitivity_torch</code> is a package designed to allow taking first- and second-order derivatives through optimization or any other fixed-point process.</p> <p>Source code for this package is located here github.com/rdyro/sensitivity_torch</p> <p>This package builds on top of PyTorch. We also maintain an implementation in JAX here.</p> <p> </p>"},{"location":"installation/","title":"Installation","text":"<p>Install using pip</p> <pre><code>$ pip install git+https://github.com/rdyro/sensitivity_torch.git\n</code></pre> <p>or from source</p> <pre><code>$ git clone git@github.com:rdyro/sensitivity_torch.git\n$ cd sensitivity_torch\n$ python3 setup.py install --user\n</code></pre>"},{"location":"installation/#testing","title":"Testing","text":"<p>Run all unit tests using</p> <pre><code>$ python3 setup.py test\n</code></pre>"},{"location":"tour/","title":"Tour of <code>sensitivity_torch</code>","text":"<p>A simple, exhaustive, example of model tuning using sensitivity analysis</p> <pre><code>import torch, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_moons\n\nfrom sensitivity_torch.sensitivity import (\n    implicit_jacobian,\n    generate_optimization_fns,\n)\nfrom sensitivity_torch.extras.optimization import (\n    minimize_agd,\n    minimize_lbfgs,\n    minimize_sqp,\n)\n\n\ndef feature_map(X, P, q):\n    Phi = torch.cos(X @ P + q)\n    return Phi\n\n\ndef optimize_fn(P, q, X, Y):\n    \"\"\"Non-differentiable optimization function using sklearn.\"\"\"\n    Phi = feature_map(X, P, q)\n    model = LogisticRegression(max_iter=300, C=1e-2)\n    model.fit(Phi, Y)\n    z = np.concatenate([model.coef_.reshape(-1), model.intercept_])\n    return torch.tensor(z)\n\n\ndef loss_fn(z, P, q, X, Y):\n    \"\"\"The measure of model performance.\"\"\"\n    Phi = feature_map(X, P, q)\n    W, b = z[:-1], z[-1]\n    prob = torch.sigmoid(Phi @ W + b)\n    loss = torch.sum(-(Y * torch.log(prob) + (1 - Y) * torch.log(1 - prob)))\n    regularization = 0.5 * torch.sum(W ** 2) / 1e-2\n    return (loss + regularization) / X.shape[-2]\n\n\ndef k_fn(z, P, q, X, Y):\n    \"\"\"Optimalty conditions of the model \u2013 the fixed-point of optimization.\"\"\"\n    return torch.autograd.functional.jacobian(\n        lambda z: loss_fn(z, P, q, X, Y), z, create_graph=True\n    )\n\n\nif __name__ == \"__main__\":\n    ######################## SETUP ####################################\n    # generate data and split into train and test sets\n    X, Y = make_moons(n_samples=1000, noise=1e-1)\n    X, Y = torch.tensor(X), torch.tensor(Y)\n    n_train = 200\n    Xtr, Ytr = X[:n_train, :], Y[:n_train]\n    Xts, Yts = X[n_train:, :], Y[n_train:]\n\n    # generate the Fourier feature map parameters Phi = cos(X @ P + q)\n    n_features = 200\n    P = torch.tensor(np.random.randn(2, n_features))\n    q = torch.tensor(np.zeros(n_features))\n\n    # check that the fixed-point is numerically close to zero\n    z = optimize_fn(P, q, X, Y)\n    k = k_fn(z, P, q, X, Y)\n    assert torch.max(torch.abs(k)) &lt; 1e-5\n\n    ######################## SENSITIVITY ##############################\n    # generate sensitivity Jacobians and check their shape\n    JP, Jq = implicit_jacobian(lambda z, P, q: k_fn(z, P, q, X, Y), z, P, q)\n    assert JP.shape == z.shape + P.shape\n    assert Jq.shape == z.shape + q.shape\n\n    ######################## OPTIMIZATION #############################\n    # generate necessary functions for optimization\n    opt_fn_ = lambda P: optimize_fn(P, q, Xtr, Ytr)  # model optimization\n    k_fn_ = lambda z, P: k_fn(z, P, q, Xtr, Ytr)  # fixed-point\n    loss_fn_ = lambda z, P: loss_fn(z, P, q, Xts, Yts)  # loss to improve\n    f_fn, g_fn, h_fn = generate_optimization_fns(loss_fn_, opt_fn_, k_fn_)\n\n    # choose any optimization routine\n    # Ps = minimize_agd(f_fn, g_fn, P, verbose=True, ai=1e-1, af=1e-1, max_it=100)\n    # Ps = minimize_lbfgs(f_fn, g_fn, P, verbose=True, lr=1e-1, max_it=10)\n    Ps = minimize_sqp(f_fn, g_fn, h_fn, P, verbose=True, max_it=10)\n\n    def predict(z, P, q, X):\n        Phi = feature_map(X, P, q)\n        W, b = z[:-1], z[-1]\n        prob = torch.sigmoid(Phi @ W + b)\n        return torch.round(prob)\n\n    # evaluate the results\n    acc0 = torch.mean(1.0 * (predict(opt_fn_(P), P, q, Xts) == Yts))\n    accf = torch.mean(1.0 * (predict(opt_fn_(Ps), Ps, q, Xts) == Yts))\n    print(\"Accuracy before: %4.2f%%\" % (1e2 * acc0))\n    print(\"Accuracy after:  %4.2f%%\" % (1e2 * accf))\n    print(\"Loss before:     %9.4e\" % loss_fn(opt_fn_(P), P, q, Xts, Yts))\n    print(\"Loss after:      %9.4e\" % loss_fn(opt_fn_(Ps), Ps, q, Xts, Yts))\n</code></pre>"},{"location":"api/misc/","title":"Misc","text":""},{"location":"api/misc/#utility-differentiation-routines","title":"Utility Differentiation Routines","text":""},{"location":"api/misc/#sensitivity_torch.differentiation","title":"<code>sensitivity_torch.differentiation</code>","text":""},{"location":"api/misc/#sensitivity_torch.differentiation.JACOBIAN","title":"<code>JACOBIAN(fn, inputs, **config)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def JACOBIAN(fn, inputs, **config):\n    config.setdefault(\"vectorize\", True)\n    return torch.autograd.functional.jacobian(fn, inputs, **config)\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.HESSIAN","title":"<code>HESSIAN(fn, inputs, **config)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def HESSIAN(fn, inputs, **config):\n    config.setdefault(\"vectorize\", True)\n    return torch.autograd.functional.hessian(fn, inputs, **config)\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.HESSIAN_CUSTOM","title":"<code>HESSIAN_CUSTOM(fn, args, **config)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def HESSIAN_CUSTOM(fn, args, **config):\n    single_input = not isinstance(args, (list, tuple))\n    args = (args,) if single_input else tuple(args)\n\n    f = fn(*args)\n    n = f.numel()\n    if n == 1:\n        return HESSIAN(fn, args, **config)\n    else:\n        Hs = [HESSIAN(lambda *args: fn(*args).reshape(-1)[i], args, **config) for i in range(n)]\n        Hs = [\n            [\n                torch.stack([Hs[k][i][j] for k in range(n)], 0).reshape(f.shape + Hs[0][i][j].shape)\n                for i in range(len(args))\n            ]\n            for j in range(len(args))\n        ]\n        return Hs\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.HESSIAN_DIAG","title":"<code>HESSIAN_DIAG(fn, args, **config)</code>","text":"<p>Generates a function which computes per-argument partial Hessians.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def HESSIAN_DIAG(fn, args, **config):\n\"\"\"Generates a function which computes per-argument partial Hessians.\"\"\"\n    single_input = not isinstance(args, (list, tuple))\n    args = (args,) if single_input else tuple(args)\n    try:\n        ret = [\n            HESSIAN(lambda arg: fn(*args[:i], arg, *args[i + 1 :]), arg, **config)\n            for (i, arg) in enumerate(args)\n        ]\n    except RuntimeError:  # function has more than 1 output, need to use JACOBIAN\n        ret = [\n            JACOBIAN(\n                lambda arg: JACOBIAN(\n                    lambda arg: fn(*args[:i], arg, *args[i + 1 :]),\n                    arg,\n                    **dict(config, create_graph=True),\n                ),\n                arg,\n                **config,\n            )\n            for (i, arg) in enumerate(args)\n        ]\n    return ret[0] if single_input else ret\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.BATCH_JACOBIAN","title":"<code>BATCH_JACOBIAN(fn, args, **config)</code>","text":"<p>Computes the Hessian, assuming the first in/out dimension is the batch.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def BATCH_JACOBIAN(fn, args, **config):\n\"\"\"Computes the Hessian, assuming the first in/out dimension is the batch.\"\"\"\n    single_input = not isinstance(args, (list, tuple))\n    args = (args,) if single_input else tuple(args)\n    Js = JACOBIAN(lambda *args: torch.sum(fn(*args), 0), args, **config)\n    out_shapes = [J.shape[: -len(arg.shape)] for (J, arg) in zip(Js, args)]\n    Js = [\n        J.reshape((prod(out_shape),) + arg.shape)\n        .swapaxes(0, 1)\n        .reshape((arg.shape[0],) + out_shape + arg.shape[1:])\n        for (J, out_shape, arg) in zip(Js, out_shapes, args)\n    ]\n    return Js[0] if single_input else Js\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.BATCH_HESSIAN","title":"<code>BATCH_HESSIAN(fn, args, **config)</code>","text":"<p>Computes the Hessian, assuming the first in/out dimension is the batch.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def BATCH_HESSIAN(fn, args, **config):\n\"\"\"Computes the Hessian, assuming the first in/out dimension is the batch.\"\"\"\n    single_input = not isinstance(args, (list, tuple))\n    args = (args,) if single_input else tuple(args)\n    assert single_input\n    return BATCH_JACOBIAN(\n        lambda *args: BATCH_JACOBIAN(fn, args, **dict(config, create_graph=True))[0],\n        args,\n        **config,\n    )[0]\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.BATCH_HESSIAN_DIAG","title":"<code>BATCH_HESSIAN_DIAG(fn, args, **config)</code>","text":"<p>Evaluates per-argument partial batch (first dimension) Hessians.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def BATCH_HESSIAN_DIAG(fn, args, **config):\n\"\"\"Evaluates per-argument partial batch (first dimension) Hessians.\"\"\"\n    single_input = not isinstance(args, (list, tuple))\n    args = (args,) if single_input else tuple(args)\n    try:\n        ret = [\n            BATCH_HESSIAN(lambda arg: fn(*args[:i], arg, *args[i + 1 :]), arg, **config)\n            for (i, arg) in enumerate(args)\n        ]\n    except RuntimeError:  # function has more than 1 output, need to use JACOBIAN\n        assert False\n        ret = [\n            BATCH_JACOBIAN(\n                lambda arg: BATCH_JACOBIAN(\n                    lambda arg: fn(*args[:i], arg, *args[i + 1 :]),\n                    arg,\n                    **dict(config, create_graph=True),\n                ),\n                arg,\n                **config,\n            )\n            for (i, arg) in enumerate(args)\n        ]\n    return ret[0] if single_input else ret\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.prod","title":"<code>prod(x)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def prod(x):\n    return functools.reduce(operator.mul, x, 1)\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.write_list_at","title":"<code>write_list_at(xs, idxs, els)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def write_list_at(xs, idxs, els):\n    assert len(idxs) == len(els)\n    k, xs = 0, [x for x in xs]\n    for idx in idxs:\n        xs[idx] = els[k]\n        k += 1\n    return xs\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.fwd_grad","title":"<code>fwd_grad(ys, xs, grad_inputs=None, **kwargs)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def fwd_grad(ys, xs, grad_inputs=None, **kwargs):\n    # we only support a single input, otherwise undesirable accumulation occurs\n    if isinstance(xs, list) or isinstance(xs, tuple):\n        assert len(xs) == 1\n\n    # select only the outputs which have a gradient path/graph\n    ys_ = [ys] if not (isinstance(ys, list) or isinstance(ys, tuple)) else ys\n    idxs = [i for (i, y) in enumerate(ys_) if y.grad_fn is not None]\n    ys_select = [ys_[i] for i in idxs]\n    if len(ys_) == 0:\n        return [torch.zeros_like(y) for y in ys_]\n\n    # perform the first step of forward mode emulation in reverse mode AD\n    vs_ = [torch.ones_like(y, requires_grad=True) for y in ys_select]\n    gs_ = torch.autograd.grad(ys_select, xs, grad_outputs=vs_, create_graph=True, allow_unused=True)\n\n    # perform the second step of reverse mode emulation in reverse mode AD\n    if grad_inputs is not None:\n        # apply the JVP if necessary\n        gs_ = torch.autograd.grad(gs_, vs_, grad_outputs=grad_inputs, allow_unused=True)\n    else:\n        gs_ = torch.autograd.grad(gs_, vs_, allow_unused=True)\n\n    # fill in the unused outputs with zeros (rather than None)\n    gs_ = [torch.zeros_like(y) if g is None else g for (g, y) in zip(gs_, ys_select)]\n\n    # fill in the outputs which did not have gradient paths\n    ret = write_list_at([torch.zeros_like(y) for y in ys_], idxs, gs_)\n\n    # return a single output if the output was a single element\n    return ret if isinstance(ys, list) or isinstance(ys, tuple) else ret[0]\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.grad","title":"<code>grad(y, xs, **kwargs)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def grad(y, xs, **kwargs):\n    gs = torch.autograd.grad(y, xs, **kwargs)\n    return gs[0] if not (isinstance(xs, list) or isinstance(xs, tuple)) else gs\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.reshape_linear","title":"<code>reshape_linear(fs)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def reshape_linear(fs):\n    if not (isinstance(fs, tuple) or isinstance(fs, list)):\n        return [fs], {\"size\": 1, \"nb\": 0}\n    else:\n        ret = [reshape_linear(f) for f in fs]\n        vals, trees = [el[0] for el in ret], [el[1] for el in ret]\n        tree = {i: trees[i] for i in range(len(trees))}\n        tree[\"size\"] = sum([tree[\"size\"] for tree in trees])\n        tree[\"nb\"] = len(trees)\n        return sum(vals, []), tree\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.reshape_struct","title":"<code>reshape_struct(fs_flat, tree)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def reshape_struct(fs_flat, tree):\n    if tree[\"nb\"] == 0:\n        return fs_flat[0]\n    k = 0\n    fs = [None for i in range(tree[\"nb\"])]\n    for i in range(tree[\"nb\"]):\n        if tree[i][\"nb\"] == 0:\n            fs[i] = fs_flat[k]\n        elif tree[i][\"nb\"] &gt; 0:\n            fs[i] = reshape_struct(fs_flat[k : k + tree[i][\"size\"]], tree[i])\n        else:\n            fs[i] = fs_flat[k : k + tree[i][\"size\"]]\n        k += tree[i][\"size\"]\n    return tuple(fs)\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.torch_grad","title":"<code>torch_grad(fn, argnums=0, bdims=0, create_graph=False, retain_graph=False, verbose=False)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def torch_grad(\n    fn,\n    argnums=0,\n    bdims=0,\n    create_graph=False,\n    retain_graph=False,\n    verbose=False,\n):\n    def g_fn(*args, **kwargs):\n        args = list(args)\n        argnums_ = list(argnums) if hasattr(argnums, \"__iter__\") else [argnums]\n        for i in range(len(args)):\n            if i in argnums_:\n                args[i] = torch.as_tensor(args[i])\n                args[i].requires_grad = True\n        gargs = [arg for (i, arg) in enumerate(args) if i in argnums_]\n        fs = fn(*args, **kwargs)\n        fs, tree = reshape_linear(fs)\n        G = [None for _ in range(len(fs))]\n        for j, f in enumerate(fs):\n            f_org_shape = f.shape\n            f = f.reshape(f.shape[:bdims] + (-1,))\n            Js = [\n                torch.zeros((f.shape[-1],) + garg.shape, dtype=f.dtype, device=f.device)\n                for garg in gargs\n            ]\n            rng = tqdm(range(f.shape[-1])) if verbose else range(f.shape[-1])\n            for i in rng:\n                f_ = torch.sum(f[..., i])\n                if f_.grad_fn is not None:\n                    gs = torch.autograd.grad(\n                        f_,\n                        gargs,\n                        create_graph=create_graph,\n                        retain_graph=(\n                            create_graph or j &lt; len(fs) - 1 or i &lt; f.shape[-1] - 1 or retain_graph\n                        ),\n                        allow_unused=True,\n                    )\n                else:\n                    gs = [None for garg in gargs]\n                gs = [\n                    g\n                    if g is not None\n                    else torch.zeros(\n                        gargs[k].shape,\n                        dtype=gargs[k].dtype,\n                        device=gargs[k].device,\n                    )\n                    for (k, g) in enumerate(gs)\n                ]\n                for k in range(len(Js)):\n                    Js[k][i, ...] = gs[k].reshape((-1,) + gs[k].shape)\n            for k, J in enumerate(Js):\n                lshp = f_org_shape[bdims:]\n                bshp = gargs[k].shape[:bdims]\n                rshp = gargs[k].shape[bdims:]\n                Js[k] = (\n                    J.reshape((prod(lshp), prod(bshp), prod(rshp)))\n                    .transpose(-2, -3)\n                    .reshape(bshp + lshp + rshp)\n                )\n            if len(Js) &gt; 1 or hasattr(argnums, \"__iter__\"):\n                G[j] = tuple(Js)\n            else:\n                G[j] = Js[0]\n\n        ret = reshape_struct(G, tree)\n        return ret\n\n    return g_fn\n</code></pre>"},{"location":"api/misc/#sensitivity_torch.differentiation.torch_hessian","title":"<code>torch_hessian(fn, argnums=0, bdims=0, create_graph=False)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def torch_hessian(fn, argnums=0, bdims=0, create_graph=False):\n    g_fn = torch_grad(fn, argnums=argnums, bdims=bdims, create_graph=True)\n    g2_fn = torch_grad(g_fn, argnums=argnums, bdims=bdims, create_graph=create_graph)\n    return g2_fn\n</code></pre>"},{"location":"api/misc/#specialized-matrix-inverses","title":"Specialized Matrix Inverses","text":""},{"location":"api/misc/#sensitivity_torch.specialized_matrix_inverse","title":"<code>sensitivity_torch.specialized_matrix_inverse</code>","text":""},{"location":"api/misc/#sensitivity_torch.specialized_matrix_inverse.solve_cg","title":"<code>solve_cg = partial(_solve_spla, spla.cg)</code>  <code>module-attribute</code>","text":""},{"location":"api/misc/#sensitivity_torch.specialized_matrix_inverse.solve_gmres","title":"<code>solve_gmres = partial(_solve_spla, spla.gmres)</code>  <code>module-attribute</code>","text":""},{"location":"api/optimization/","title":"Optimization","text":""},{"location":"api/optimization/#generating-bilevel-optimization-functions","title":"Generating Bilevel Optimization Functions","text":""},{"location":"api/optimization/#sensitivity_torch.sensitivity.generate_optimization_fns","title":"<code>sensitivity_torch.sensitivity.generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> required <p>Returns:</p> Type Description <code>Tuple[Callable, Callable, Callable]</code> <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code> parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_torch/sensitivity.py</code> <pre><code>def generate_optimization_fns(\n    loss_fn: Callable,\n    opt_fn: Callable,\n    k_fn: Callable,\n    normalize_grad: bool = False,\n    optimizations: Mapping = None,\n) -&gt; Tuple[Callable, Callable, Callable]:\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)`` parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\n    sol_cache = dict()\n    opt_fn_ = lambda *args, **kwargs: opt_fn(*args, **kwargs).detach()\n    optimizations = {} if optimizations is None else copy(optimizations)\n\n    @fn_with_sol_cache(opt_fn_, sol_cache)\n    def f_fn(z, *params):\n        z = z.detach() if isinstance(z, torch.Tensor) else z\n        params = _detach_args(*params)\n        return loss_fn(z, *params)\n\n    @fn_with_sol_cache(opt_fn_, sol_cache)\n    def g_fn(z, *params):\n        z = z.detach() if isinstance(z, torch.Tensor) else z\n        params = _detach_args(*params)\n        g = JACOBIAN(loss_fn, (z, *params))\n        Dp = implicit_jacobian(k_fn, z.detach(), *params, Dg=g[0], optimizations=optimizations)\n        Dp = Dp if len(params) != 1 else [Dp]\n        # opts = dict(device=z.device, dtype=z.dtype)\n        # Dp = [\n        #    torch.zeros(param.shape, **opts) for param in params\n        # ]\n        ret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\n        if normalize_grad:\n            ret = [(z / (torch.norm(z) + 1e-7)).detach() for z in ret]\n        ret = [ret.detach() for ret in ret]\n        return ret[0] if len(ret) == 1 else ret\n\n    @fn_with_sol_cache(opt_fn_, sol_cache)\n    def h_fn(z, *params):\n        z = z.detach() if isinstance(z, torch.Tensor) else z\n        params = _detach_args(*params)\n        g = JACOBIAN(loss_fn, (z, *params))\n\n        if optimizations.get(\"Hz_fn\", None) is None:\n            optimizations[\"Hz_fn\"] = lambda z, *params: HESSIAN_DIAG(\n                lambda z: loss_fn(z, *params), (z,)\n            )[0]\n        Hz_fn = optimizations[\"Hz_fn\"]\n        Hz = Hz_fn(z, *params)\n        H = [Hz] + HESSIAN_DIAG(lambda *params: loss_fn(z, *params), params)\n\n        Dp, Dpp = implicit_hessian(\n            k_fn,\n            z,\n            *params,\n            Dg=g[0],\n            Hg=H[0],\n            optimizations=optimizations,\n        )\n        Dpp = Dpp if len(params) != 1 else [Dpp]\n        ret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\n        ret = [ret.detach() for ret in ret]\n        return ret[0] if len(ret) == 1 else ret\n\n    return f_fn, g_fn, h_fn\n</code></pre>"},{"location":"api/optimization/#with-batch-sensitivity","title":"...with Batch Sensitivity","text":""},{"location":"api/optimization/#sensitivity_torch.batch_sensitivity.generate_optimization_fns","title":"<code>sensitivity_torch.batch_sensitivity.generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> required <p>Returns:</p> Type Description <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code> parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_torch/batch_sensitivity.py</code> <pre><code>def generate_optimization_fns(\n    loss_fn: Callable,\n    opt_fn: Callable,\n    k_fn: Callable,\n    normalize_grad: bool = False,\n    optimizations: Mapping = None,\n):\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)`` parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\n    sol_cache = dict()\n    opt_fn_ = lambda *args, **kwargs: opt_fn(*args, **kwargs).detach()\n    optimizations = {} if optimizations is None else copy(optimizations)\n\n    @fn_with_sol_cache(opt_fn_, sol_cache)\n    def f_fn(z, *params):\n        z = z.detach() if isinstance(z, Tensor) else z\n        params = _detach_args(*params)\n        return loss_fn(z, *params)\n\n    @fn_with_sol_cache(opt_fn_, sol_cache)\n    def g_fn(z, *params):\n        z = z.detach() if isinstance(z, Tensor) else z\n        params = _detach_args(*params)\n        g = JACOBIAN(loss_fn, (z, *params))\n        Dp = implicit_jacobian(k_fn, z.detach(), *params, Dg=g[0], optimizations=optimizations)\n        Dp = Dp if len(params) != 1 else [Dp]\n        ret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\n        if normalize_grad:\n            ret = [(z / (torch.norm(z) + 1e-7)).detach() for z in ret]\n        ret = [ret.detach() for ret in ret]\n        return ret[0] if len(ret) == 1 else ret\n\n    @fn_with_sol_cache(opt_fn_, sol_cache)\n    def h_fn(z, *params):\n        z = z.detach() if isinstance(z, Tensor) else z\n        params = _detach_args(*params)\n        g = JACOBIAN(loss_fn, (z, *params))\n\n        if optimizations.get(\"Hz_fn\", None) is None:\n            optimizations[\"Hz_fn\"] = lambda z, *params: BATCH_HESSIAN_DIAG(\n                lambda z: loss_fn(z, *params), (z,)\n            )[0]\n        Hz_fn = optimizations[\"Hz_fn\"]\n        Hz = Hz_fn(z, *params)\n        H = [Hz] + BATCH_HESSIAN_DIAG(lambda *params: loss_fn(z, *params), params)\n\n        Dp, Dpp = implicit_hessian(\n            k_fn,\n            z,\n            *params,\n            Dg=g[0],\n            Hg=H[0],\n            optimizations=optimizations,\n        )\n        Dpp = Dpp if len(params) != 1 else [Dpp]\n        ret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\n        ret = [ret.detach() for ret in ret]\n        return ret[0] if len(ret) == 1 else ret\n\n    return f_fn, g_fn, h_fn\n</code></pre>"},{"location":"api/sensitivity/","title":"Sensitivity","text":""},{"location":"api/sensitivity/#sensitivity_1","title":"Sensitivity","text":""},{"location":"api/sensitivity/#sensitivity_torch.sensitivity.implicit_jacobian","title":"<code>sensitivity_torch.sensitivity.implicit_jacobian(k_fn, z, *params, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_torch/sensitivity.py</code> <pre><code>def implicit_jacobian(\n    k_fn: Callable,\n    z: Tensor,\n    *params: Tensor,\n    Dg: Tensor = None,\n    jvp_vec: Union[Tensor, Sequence[Tensor]] = None,\n    matrix_free_inverse: bool = False,\n    full_output: bool = False,\n    optimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.\n\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\n    optimizations = {} if optimizations is None else copy(optimizations)\n    zlen, plen = _prod(z.shape), [_prod(param.shape) for param in params]\n    jvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\n\n    # construct a default Dzk_solve_fn ##########################\n    if optimizations.get(\"Dzk_solve_fn\", None) is None:\n        _generate_default_Dzk_solve_fn(optimizations, k_fn)\n    #############################################################\n\n    if Dg is not None:\n        if matrix_free_inverse:\n            A_fn = lambda x: JACOBIAN(\n                lambda z: torch.sum(k_fn(z, *params).reshape(-1) * x.reshape(-1)),\n                z,\n            ).reshape(x.shape)\n            v = -solve_gmres(A_fn, Dg.reshape((zlen, 1)), max_it=300)\n        else:\n            Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n            v = -Dzk_solve_fn(z, *params, rhs=Dg.reshape((zlen, 1)), T=True)\n        v = v.detach()\n        fn = lambda *params: torch.sum(v.reshape(zlen) * k_fn(z, *params).reshape(zlen))\n        Dp = JACOBIAN(fn, params)\n        Dp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\n        ret = Dp_shaped[0] if len(params) == 1 else Dp_shaped\n    else:\n        if jvp_vec is not None:\n            for param in params:\n                param.requires_grad = True\n            f_ = k_fn(z.detach(), *params)\n            Dp = [\n                fwd_grad(f_, param, grad_inputs=jvp_vec)\n                for (param, jvp_vec) in zip(params, jvp_vec)\n            ]\n            Dp = [Dp.reshape((zlen, 1)) for (Dp, plen) in zip(Dp, plen)]\n            Dpk = Dp\n        else:\n            Dpk = JACOBIAN(lambda *params: k_fn(z, *params), params)\n            Dpk = [Dpk.reshape((zlen, plen)) for (Dpk, plen) in zip(Dpk, plen)]\n\n        Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n        Dpz = [-Dzk_solve_fn(z, *params, rhs=Dpk, T=False) for Dpk in Dpk]\n\n        if jvp_vec is not None:\n            Dpz_shaped = [Dpz.reshape(z.shape) for Dpz in Dpz]\n        else:\n            Dpz_shaped = [Dpz.reshape(z.shape + param.shape) for (Dpz, param) in zip(Dpz, params)]\n        ret = Dpz_shaped if len(params) != 1 else Dpz_shaped[0]\n    return (ret, optimizations) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity/#sensitivity_torch.sensitivity.implicit_hessian","title":"<code>sensitivity_torch.sensitivity.implicit_hessian(k_fn, z, *params, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>Tensor</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_torch/sensitivity.py</code> <pre><code>def implicit_hessian(\n    k_fn: Callable,\n    z: Tensor,\n    *params: Tensor,\n    Dg: Tensor = None,\n    Hg: Tensor = None,\n    jvp_vec: Union[Tensor, Sequence[Tensor]] = None,\n    optimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.\n\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\n    optimizations = {} if optimizations is None else copy(optimizations)\n    zlen, plen = _prod(z.shape), [_prod(param.shape) for param in params]\n    jvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\n    if jvp_vec is not None:\n        assert Dg is not None\n\n    # construct a default Dzk_solve_fn ##########################\n    if optimizations.get(\"Dzk_solve_fn\", None) is None:\n        _generate_default_Dzk_solve_fn(optimizations, k_fn)\n    #############################################################\n\n    # compute 2nd implicit gradients\n    if Dg is not None:\n        assert Dg.numel() == zlen\n        assert Hg is None or Hg.numel() == zlen**2\n\n        Dg_ = Dg.reshape((zlen, 1))\n        Hg_ = Hg.reshape((zlen, zlen)) if Hg is not None else Hg\n\n        # compute the left hand vector in the VJP\n        Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n        v = -Dzk_solve_fn(z, *params, rhs=Dg_.reshape((zlen, 1)), T=True)\n        v = v.detach()\n        fn = lambda z, *params: torch.sum(v.reshape(zlen) * k_fn(z, *params).reshape(zlen))\n\n        if jvp_vec is not None:\n            for param in params:\n                param.requires_grad = True\n            z.requires_grad = True\n\n            Dpz_jvp = _ensure_list(\n                implicit_jacobian(\n                    k_fn,\n                    z,\n                    *params,\n                    jvp_vec=jvp_vec,\n                    optimizations=optimizations,\n                )\n            )\n            Dpz_jvp = [Dpz_jvp.reshape(-1).detach() for Dpz_jvp in Dpz_jvp]\n\n            # compute the 2nd order derivatives consisting of 4 terms\n            # term 1 ##############################\n            # Dpp1 = HESSIAN_DIAG(lambda *params: fn(z, *params), *params)\n            g_ = grad(fn(z, *params), params, create_graph=True)\n            Dpp1 = [\n                fwd_grad(g_, param, grad_inputs=jvp_vec).reshape(plen)\n                for (g_, param, jvp_vec) in zip(g_, params, jvp_vec)\n            ]\n\n            # term 2 ##############################\n            # temp = JACOBIAN(\n            #    lambda z: JACOBIAN(\n            #        lambda *params: fn(z, *params), *params, create_graph=True\n            #    ),\n            #    z,\n            # )\n            # temp = [temp] if len(params) == 1 else temp\n            # temp = [\n            #    temp.reshape((plen, zlen)) for (temp, plen) in zip(temp, plen)\n            # ]\n            # Dpp2 = [\n            #    (temp @ Dpz).reshape((plen, plen))\n            #    for (temp, Dpz, plen) in zip(temp, Dpz, plen)\n            # ]\n            g_ = grad(fn(z, *params), params, create_graph=True)\n            Dpp2 = [\n                fwd_grad(g_, z, grad_inputs=Dpz_jvp.reshape(z.shape)).reshape(-1)\n                for (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n            ]\n\n            # term 3 ##############################\n            # Dpp3 = [t(Dpp2) for Dpp2 in Dpp2]\n            g_ = grad(fn(z, *params), z, create_graph=True)\n            g_ = [\n                fwd_grad(g_, param, grad_inputs=jvp_vec)\n                for (param, jvp_vec) in zip(params, jvp_vec)\n            ]\n            Dpp3 = [\n                _ensure_list(\n                    implicit_jacobian(\n                        k_fn,\n                        z,\n                        *params,\n                        Dg=g_,\n                        optimizations=optimizations,\n                    )\n                )[i].reshape(-1)\n                for (i, g_) in enumerate(g_)\n            ]\n\n            # term 4 ##############################\n            # Dzz = HESSIAN(lambda z: fn(z, *params), z).reshape((zlen, zlen))\n            # if Hg is not None:\n            #    Dpp4 = [t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\n            # else:\n            #    Dpp4 = [t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\n            g_ = grad(fn(z, *params), z, create_graph=True)\n            g_ = [fwd_grad(g_, z, grad_inputs=Dpz_jvp.reshape(z.shape)) for Dpz_jvp in Dpz_jvp]\n            if Hg is not None:\n                g_ = [\n                    g_.reshape(zlen) + Hg_ @ Dpz_jvp.reshape(zlen)\n                    for (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n                ]\n            Dpp4 = [\n                _ensure_list(\n                    implicit_jacobian(\n                        k_fn,\n                        z,\n                        *params,\n                        Dg=g_,\n                        optimizations=optimizations,\n                    )\n                )[i].reshape(plen)\n                for ((i, g_), plen) in zip(enumerate(g_), plen)\n            ]\n            Dp = [Dg_.reshape((1, zlen)) @ Dpz_jvp.reshape(zlen) for Dpz_jvp in Dpz_jvp]\n            Dpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n\n            # return the results\n            Dp_shaped = [Dp.reshape(()) for Dp in Dp]\n            Dpp_shaped = [Dpp.reshape(param.shape) for (Dpp, param) in zip(Dpp, params)]\n        else:\n            # compute the full first order 1st gradients\n            Dpz = _ensure_list(\n                implicit_jacobian(\n                    k_fn,\n                    z,\n                    *params,\n                    optimizations=optimizations,\n                )\n            )\n            Dpz = [Dpz.reshape((zlen, plen)).detach() for (Dpz, plen) in zip(Dpz, plen)]\n\n            # compute the 2nd order derivatives consisting of 4 terms\n            Dpp1 = HESSIAN_DIAG(lambda *params: fn(z, *params), params)\n            Dpp1 = [Dpp1.reshape((plen, plen)) for (Dpp1, plen) in zip(Dpp1, plen)]\n\n            # temp = JACOBIAN(\n            #    lambda z: JACOBIAN(\n            #        lambda *params: fn(z, *params), params, create_graph=True\n            #    ),\n            #    z,\n            # )\n            temp = JACOBIAN(\n                lambda *params: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\n                params,\n            )\n            temp = [\n                temp.reshape((zlen, plen)).transpose(-1, -2) for (temp, plen) in zip(temp, plen)\n            ]\n            Dpp2 = [\n                (temp @ Dpz).reshape((plen, plen)) for (temp, Dpz, plen) in zip(temp, Dpz, plen)\n            ]\n            Dpp3 = [t(Dpp2) for Dpp2 in Dpp2]\n            Dzz = HESSIAN(lambda z: fn(z, *params), z).reshape((zlen, zlen))\n            if Hg is not None:\n                Dpp4 = [t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\n            else:\n                Dpp4 = [t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\n            Dp = [Dg_.reshape((1, zlen)) @ Dpz for Dpz in Dpz]\n            Dpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n\n            # return the results\n            Dp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\n            Dpp_shaped = [\n                Dpp.reshape(param.shape + param.shape) for (Dpp, param) in zip(Dpp, params)\n            ]\n        return (Dp_shaped[0], Dpp_shaped[0]) if len(params) == 1 else (Dp_shaped, Dpp_shaped)\n    else:\n        Dpz, optimizations = implicit_jacobian(\n            k_fn,\n            z,\n            *params,\n            full_output=True,\n            optimizations=optimizations,\n        )\n        Dpz = _ensure_list(Dpz)\n        Dpz = [Dpz.reshape(zlen, plen) for (Dpz, plen) in zip(Dpz, plen)]\n\n        # compute derivatives\n        if optimizations.get(\"Dzzk\", None) is None:\n            Hk = HESSIAN_DIAG(k_fn, (z, *params))\n            Dzzk, Dppk = Hk[0], Hk[1:]\n            optimizations[\"Dzzk\"] = Dzzk\n        else:\n            Dppk = HESSIAN_DIAG(lambda *params: k_fn(z, *params), params)\n        Dzpk = JACOBIAN(\n            lambda *params: JACOBIAN(lambda z: k_fn(z, *params), z, create_graph=True),\n            params,\n        )\n        Dppk = [Dppk.reshape((zlen, plen, plen)) for (Dppk, plen) in zip(Dppk, plen)]\n        Dzzk = Dzzk.reshape((zlen, zlen, zlen))\n        Dzpk = [Dzpk.reshape((zlen, zlen, plen)) for (Dzpk, plen) in zip(Dzpk, plen)]\n        Dpzk = [Dzpk.transpose(-1, -2) for Dzpk in Dzpk]\n\n        # solve the IFT equation\n        lhs = [\n            Dppk\n            + Dpzk @ Dpz[None, ...]\n            + t(Dpz)[None, ...] @ Dzpk\n            + (t(Dpz)[None, ...] @ Dzzk) @ Dpz[None, ...]\n            for (Dpz, Dzpk, Dpzk, Dppk) in zip(Dpz, Dzpk, Dpzk, Dppk)\n        ]\n        Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n        Dppz = [\n            -Dzk_solve_fn(z, *params, rhs=lhs.reshape((zlen, plen * plen)), T=False).reshape(\n                (zlen, plen, plen)\n            )\n            for (lhs, plen) in zip(lhs, plen)\n        ]\n\n        # return computed values\n        Dpz_shaped = [Dpz.reshape(z.shape + param.shape) for (Dpz, param) in zip(Dpz, params)]\n        Dppz_shaped = [\n            Dppz.reshape(z.shape + param.shape + param.shape) for (Dppz, param) in zip(Dppz, params)\n        ]\n        return (Dpz_shaped[0], Dppz_shaped[0]) if len(params) == 1 else (Dpz_shaped, Dppz_shaped)\n</code></pre>"},{"location":"api/sensitivity/#batch-sensitvity","title":"Batch Sensitvity","text":""},{"location":"api/sensitivity/#sensitivity_torch.batch_sensitivity.implicit_jacobian","title":"<code>sensitivity_torch.batch_sensitivity.implicit_jacobian(k_fn, z, *params, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_torch/batch_sensitivity.py</code> <pre><code>def implicit_jacobian(\n    k_fn: Callable,\n    z: Tensor,\n    *params: Tensor,\n    Dg: Tensor = None,\n    jvp_vec: Union[Tensor, Sequence[Tensor]] = None,\n    matrix_free_inverse: bool = False,\n    full_output: bool = False,\n    optimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.\n\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\n    optimizations = {} if optimizations is None else copy(optimizations)\n    blen, zlen = z.shape[0], _prod(z.shape[1:])\n    plen = [_prod(param.shape[1:]) for param in params]\n    jvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\n\n    # construct a default Dzk_solve_fn ##########################\n    if optimizations.get(\"Dzk_solve_fn\", None) is None:\n        _generate_default_Dzk_solve_fn(optimizations, k_fn)\n    #############################################################\n\n    if Dg is not None:\n        if matrix_free_inverse:\n            raise NotImplementedError\n        else:\n            Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n            v = -Dzk_solve_fn(z, *params, rhs=Dg.reshape((blen, zlen, 1)), T=True)\n        v = v.detach()\n        fn = lambda *params: torch.sum(\n            v.reshape((blen, zlen)) * k_fn(z, *params).reshape((blen, zlen))\n        )\n        Dp = JACOBIAN(fn, params)\n        Dp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\n        ret = Dp_shaped[0] if len(params) == 1 else Dp_shaped\n    else:\n        if jvp_vec is not None:\n            Dp = _ensure_list(\n                torch.autograd.functional.jvp(\n                    lambda *params: k_fn(z, *params),\n                    tuple(params),\n                    tuple(jvp_vec),\n                )[1]\n            )\n            Dp = [Dp.reshape((blen, zlen, 1)) for (Dp, plen) in zip(Dp, plen)]\n            Dpk = Dp\n        else:\n            Dpk = _ensure_list(BATCH_JACOBIAN(lambda *params: k_fn(z, *params), params))\n            Dpk = [Dpk.reshape((blen, zlen, plen)) for (Dpk, plen) in zip(Dpk, plen)]\n\n        Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n        Dpz = [-Dzk_solve_fn(z, *params, rhs=Dpk, T=False) for Dpk in Dpk]\n\n        if jvp_vec is not None:\n            Dpz_shaped = [Dpz.reshape(z.shape) for Dpz in Dpz]\n        else:\n            Dpz_shaped = [\n                Dpz.reshape((blen,) + z.shape[1:] + param.shape[1:])\n                for (Dpz, param) in zip(Dpz, params)\n            ]\n        ret = Dpz_shaped if len(params) != 1 else Dpz_shaped[0]\n    return (ret, optimizations) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity/#sensitivity_torch.batch_sensitivity.implicit_hessian","title":"<code>sensitivity_torch.batch_sensitivity.implicit_hessian(k_fn, z, *params, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>Tensor</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_torch/batch_sensitivity.py</code> <pre><code>def implicit_hessian(\n    k_fn: Callable,\n    z: Tensor,\n    *params: Tensor,\n    Dg: Tensor = None,\n    Hg: Tensor = None,\n    jvp_vec: Union[Tensor, Sequence[Tensor]] = None,\n    optimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.\n\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\n    optimizations = {} if optimizations is None else copy(optimizations)\n    blen, zlen = z.shape[0], _prod(z.shape[1:])\n    plen = [_prod(param.shape[1:]) for param in params]\n    jvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\n    if jvp_vec is not None:\n        assert Dg is not None\n\n    # construct a default Dzk_solve_fn ##########################\n    if optimizations.get(\"Dzk_solve_fn\", None) is None:\n        _generate_default_Dzk_solve_fn(optimizations, k_fn)\n    #############################################################\n\n    # compute 2nd implicit gradients\n    if Dg is not None:\n        assert Dg.numel() == blen * zlen\n        assert Hg is None or Hg.numel() == blen * zlen**2\n\n        Dg_ = Dg.reshape((blen, zlen, 1))\n        Hg_ = Hg.reshape((blen, zlen, zlen)) if Hg is not None else Hg\n\n        # compute the left hand vector in the VJP\n        Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n        v = -Dzk_solve_fn(z, *params, rhs=Dg_.reshape((blen, zlen, 1)), T=True)\n        v = v.detach()\n        fn = lambda z, *params: torch.sum(\n            v.reshape((blen, zlen)) * k_fn(z, *params).reshape((blen, zlen))\n        )\n\n        if jvp_vec is not None:\n            Dpz_jvp = _ensure_list(\n                implicit_jacobian(\n                    k_fn,\n                    z,\n                    *params,\n                    jvp_vec=jvp_vec,\n                    optimizations=optimizations,\n                )\n            )\n            Dpz_jvp = [Dpz_jvp.reshape((blen, -1)).detach() for Dpz_jvp in Dpz_jvp]\n\n            # compute the 2nd order derivatives consisting of 4 terms\n            # term 1 ##############################\n            Dpp1 = [\n                torch.autograd.functional.jvp(\n                    lambda param: JACOBIAN(\n                        lambda param: fn(z, *params[:i], param, *params[i + 1 :]),\n                        param,\n                        create_graph=True,\n                    ),\n                    param,\n                    jvp_vec[i],\n                )[1].reshape((blen, -1))\n                for (i, param) in enumerate(params)\n            ]\n\n            # term 2 ##############################\n            Dpp2 = [\n                torch.autograd.functional.jvp(\n                    lambda z: BATCH_JACOBIAN(\n                        lambda param: fn(z, *params[:i], param, *params[i + 1 :]),\n                        params[i],\n                        create_graph=True,\n                    ),\n                    z,\n                    Dpz_jvp.reshape(z.shape),\n                )[1].reshape((blen, -1))\n                for (i, (plen, Dpz_jvp)) in enumerate(zip(plen, Dpz_jvp))\n            ]\n\n            # term 3 ##############################\n            g_ = _ensure_list(\n                torch.autograd.functional.jvp(\n                    lambda *params: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\n                    params,\n                    tuple(jvp_vec),\n                )[1]\n            )\n            Dpp3 = [\n                _ensure_list(\n                    implicit_jacobian(\n                        k_fn,\n                        z,\n                        *params,\n                        Dg=g_,\n                        optimizations=optimizations,\n                    )\n                )[i].reshape((blen, -1))\n                for (i, g_) in enumerate(g_)\n            ]\n\n            # term 4 ##############################\n            g_ = [\n                torch.autograd.functional.jvp(\n                    lambda z: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\n                    z,\n                    Dpz_jvp.reshape(z.shape),\n                )[1]\n                for Dpz_jvp in Dpz_jvp\n            ]\n            if Hg is not None:\n                g_ = [\n                    g_.reshape((blen, zlen, 1)) + Hg_ @ Dpz_jvp.reshape((blen, zlen, 1))\n                    for (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n                ]\n\n            Dpp4 = [\n                _ensure_list(\n                    implicit_jacobian(\n                        k_fn,\n                        z,\n                        *params,\n                        Dg=g_,\n                        optimizations=optimizations,\n                    )\n                )[i].reshape((blen, -1))\n                for ((i, g_), plen) in zip(enumerate(g_), plen)\n            ]\n            Dp = [\n                Dg_.reshape((blen, 1, zlen)) @ Dpz_jvp.reshape((blen, zlen, 1))\n                for Dpz_jvp in Dpz_jvp\n            ]\n            Dpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n\n            # return the results\n            Dp_shaped = [Dp.reshape((blen,)) for Dp in Dp]\n            Dpp_shaped = [Dpp.reshape(param.shape) for (Dpp, param) in zip(Dpp, params)]\n        else:\n            # compute the full first order 1st gradients\n            Dpz = _ensure_list(\n                implicit_jacobian(\n                    k_fn,\n                    z,\n                    *params,\n                    optimizations=optimizations,\n                )\n            )\n            Dpz = [Dpz.reshape((blen, zlen, plen)).detach() for (Dpz, plen) in zip(Dpz, plen)]\n\n            # compute the 2nd order derivatives consisting of 4 terms\n            Dpp1 = BATCH_HESSIAN_DIAG(lambda *params: fn(z, *params), params)\n            Dpp1 = [Dpp1.reshape((blen, plen, plen)) for (Dpp1, plen) in zip(Dpp1, plen)]\n\n            temp = BATCH_JACOBIAN(\n                lambda *params: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\n                params,\n            )\n            Dpp2 = [\n                (t(temp.reshape((blen, zlen, plen))) @ Dpz).reshape((blen, plen, plen))\n                for (temp, Dpz, plen) in zip(temp, Dpz, plen)\n            ]\n            Dpp3 = [t(Dpp2) for Dpp2 in Dpp2]\n            Dzz = BATCH_HESSIAN(lambda z: fn(z, *params), z).reshape((blen, zlen, zlen))\n            if Hg is not None:\n                Dpp4 = [t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\n            else:\n                Dpp4 = [t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\n            Dp = [Dg_.reshape((blen, 1, zlen)) @ Dpz for Dpz in Dpz]\n            Dpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n\n            # return the results\n            Dp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\n            Dpp_shaped = [\n                Dpp.reshape((blen,) + 2 * param.shape[1:]) for (Dpp, param) in zip(Dpp, params)\n            ]\n        return (Dp_shaped[0], Dpp_shaped[0]) if len(params) == 1 else (Dp_shaped, Dpp_shaped)\n    else:\n        Dpz, optimizations = implicit_jacobian(\n            k_fn,\n            z,\n            *params,\n            full_output=True,\n            optimizations=optimizations,\n        )\n        Dpz = _ensure_list(Dpz)\n        Dpz = [Dpz.reshape((blen, zlen, plen)) for (Dpz, plen) in zip(Dpz, plen)]\n\n        # compute derivatives\n        if optimizations.get(\"Dzzk\", None) is None:\n            Hk = BATCH_HESSIAN_DIAG(k_fn, (z, *params))\n            Dzzk, Dppk = Hk[0], Hk[1:]\n            optimizations[\"Dzzk\"] = Dzzk\n        else:\n            Dppk = BATCH_HESSIAN_DIAG(lambda *params: k_fn(z, *params), params)\n        Dppk = [Dppk.reshape((blen, zlen, plen, plen)) for (Dppk, plen) in zip(Dppk, plen)]\n        Dzpk = BATCH_JACOBIAN(\n            lambda *params: BATCH_JACOBIAN(lambda z: k_fn(z, *params), z, create_graph=True),\n            params,\n        )\n        Dzzk = Dzzk.reshape((blen, zlen, zlen, zlen))\n        Dzpk = [Dzpk.reshape((blen, zlen, zlen, plen)) for (Dzpk, plen) in zip(Dzpk, plen)]\n        Dpzk = [t(Dzpk) for Dzpk in Dzpk]\n\n        # solve the IFT equation\n        lhs = [\n            Dppk\n            + Dpzk @ Dpz[:, None, ...]\n            + t(Dpz)[:, None, ...] @ Dzpk\n            + (t(Dpz)[:, None, ...] @ Dzzk) @ Dpz[:, None, ...]\n            for (Dpz, Dzpk, Dpzk, Dppk) in zip(Dpz, Dzpk, Dpzk, Dppk)\n        ]\n        Dzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\n        Dppz = [\n            -Dzk_solve_fn(z, *params, rhs=lhs.reshape((blen, zlen, plen * plen)), T=False).reshape(\n                (blen, zlen, plen, plen)\n            )\n            for (lhs, plen) in zip(lhs, plen)\n        ]\n\n        # return computed values\n        Dpz_shaped = [\n            Dpz.reshape((blen,) + z.shape[1:] + param.shape[1:])\n            for (Dpz, param) in zip(Dpz, params)\n        ]\n        Dppz_shaped = [\n            Dppz.reshape((blen,) + z.shape[1:] + 2 * param.shape[1:])\n            for (Dppz, param) in zip(Dppz, params)\n        ]\n        return (Dpz_shaped[0], Dppz_shaped[0]) if len(params) == 1 else (Dpz_shaped, Dppz_shaped)\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter","title":"<code>sensitivity_torch.utils.TablePrinter</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>class TablePrinter:\n    def __init__(self, names, fmts=None, prefix=\"\", use_writer=False):\n        self.names = names\n        self.fmts = fmts if fmts is not None else [\"%9.4e\" for _ in names]\n        self.widths = [max(self.calc_width(fmt), len(name)) + 2 for (fmt, name) in zip(fmts, names)]\n        self.prefix = prefix\n        self.writer = None\n        if use_writer:\n            try:\n                from torch.utils.tensorboard import SummaryWriter\n\n                self.writer = SummaryWriter(flush_secs=1)\n                self.iteration = 0\n            except NameError:\n                print(\"SummaryWriter not available, ignoring\")\n\n    def calc_width(self, fmt):\n        f = fmt[-1]\n        width = None\n        if f == \"f\" or f == \"e\" or f == \"d\" or f == \"i\":\n            width = max(len(fmt % 1), len(fmt % (-1)))\n        elif f == \"s\":\n            width = len(fmt % \"\")\n        else:\n            raise ValueError(\"I can't recognized the [%s] print format\" % fmt)\n        return width\n\n    def pad_field(self, s, width, lj=True):\n        # lj -&gt; left justify\n        assert len(s) &lt;= width\n        rem = width - len(s)\n        if lj:\n            return (\" \" * (rem // 2)) + s + (\" \" * ((rem // 2) + (rem % 2)))\n        else:\n            return (\" \" * ((rem // 2) + (rem % 2))) + s + (\" \" * (rem // 2))\n\n    def make_row_sep(self):\n        return \"+\" + \"\".join([(\"-\" * width) + \"+\" for width in self.widths])\n\n    def make_header(self):\n        s = self.prefix + self.make_row_sep() + \"\\n\"\n        s += self.prefix\n        for name, width in zip(self.names, self.widths):\n            s += \"|\" + self.pad_field(\"%s\" % name, width, lj=True)\n        s += \"|\\n\"\n        return s + self.prefix + self.make_row_sep()\n\n    def make_footer(self):\n        return self.prefix + self.make_row_sep()\n\n    def make_values(self, vals):\n        assert len(vals) == len(self.fmts)\n        s = self.prefix + \"\"\n        for val, fmt, width in zip(vals, self.fmts, self.widths):\n            s += \"|\" + self.pad_field(fmt % val, width, lj=False)\n        s += \"|\"\n\n        if self.writer is not None:\n            for name, val in zip(self.names, vals):\n                self.writer.add_scalar(name, val, self.iteration)\n            self.iteration += 1\n\n        return s\n\n    def print_header(self):\n        print(self.make_header())\n\n    def print_footer(self):\n        print(self.make_footer())\n\n    def print_values(self, vals):\n        print(self.make_values(vals))\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.names","title":"<code>names = names</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.fmts","title":"<code>fmts = fmts if fmts is not None else ['%9.4e' for _ in names]</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.widths","title":"<code>widths = [max(self.calc_width(fmt), len(name)) + 2 for (fmt, name) in zip(fmts, names)]</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.prefix","title":"<code>prefix = prefix</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.writer","title":"<code>writer = SummaryWriter(flush_secs=1)</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.iteration","title":"<code>iteration = 0</code>  <code>instance-attribute</code>","text":""},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.__init__","title":"<code>__init__(names, fmts=None, prefix='', use_writer=False)</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def __init__(self, names, fmts=None, prefix=\"\", use_writer=False):\n    self.names = names\n    self.fmts = fmts if fmts is not None else [\"%9.4e\" for _ in names]\n    self.widths = [max(self.calc_width(fmt), len(name)) + 2 for (fmt, name) in zip(fmts, names)]\n    self.prefix = prefix\n    self.writer = None\n    if use_writer:\n        try:\n            from torch.utils.tensorboard import SummaryWriter\n\n            self.writer = SummaryWriter(flush_secs=1)\n            self.iteration = 0\n        except NameError:\n            print(\"SummaryWriter not available, ignoring\")\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.calc_width","title":"<code>calc_width(fmt)</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def calc_width(self, fmt):\n    f = fmt[-1]\n    width = None\n    if f == \"f\" or f == \"e\" or f == \"d\" or f == \"i\":\n        width = max(len(fmt % 1), len(fmt % (-1)))\n    elif f == \"s\":\n        width = len(fmt % \"\")\n    else:\n        raise ValueError(\"I can't recognized the [%s] print format\" % fmt)\n    return width\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.pad_field","title":"<code>pad_field(s, width, lj=True)</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def pad_field(self, s, width, lj=True):\n    # lj -&gt; left justify\n    assert len(s) &lt;= width\n    rem = width - len(s)\n    if lj:\n        return (\" \" * (rem // 2)) + s + (\" \" * ((rem // 2) + (rem % 2)))\n    else:\n        return (\" \" * ((rem // 2) + (rem % 2))) + s + (\" \" * (rem // 2))\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.make_row_sep","title":"<code>make_row_sep()</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def make_row_sep(self):\n    return \"+\" + \"\".join([(\"-\" * width) + \"+\" for width in self.widths])\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.make_header","title":"<code>make_header()</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def make_header(self):\n    s = self.prefix + self.make_row_sep() + \"\\n\"\n    s += self.prefix\n    for name, width in zip(self.names, self.widths):\n        s += \"|\" + self.pad_field(\"%s\" % name, width, lj=True)\n    s += \"|\\n\"\n    return s + self.prefix + self.make_row_sep()\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.make_footer","title":"<code>make_footer()</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def make_footer(self):\n    return self.prefix + self.make_row_sep()\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.make_values","title":"<code>make_values(vals)</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def make_values(self, vals):\n    assert len(vals) == len(self.fmts)\n    s = self.prefix + \"\"\n    for val, fmt, width in zip(vals, self.fmts, self.widths):\n        s += \"|\" + self.pad_field(fmt % val, width, lj=False)\n    s += \"|\"\n\n    if self.writer is not None:\n        for name, val in zip(self.names, vals):\n            self.writer.add_scalar(name, val, self.iteration)\n        self.iteration += 1\n\n    return s\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.print_header","title":"<code>print_header()</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def print_header(self):\n    print(self.make_header())\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.print_footer","title":"<code>print_footer()</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def print_footer(self):\n    print(self.make_footer())\n</code></pre>"},{"location":"api/utils/#sensitivity_torch.utils.TablePrinter.print_values","title":"<code>print_values(vals)</code>","text":"Source code in <code>sensitivity_torch/utils.py</code> <pre><code>def print_values(self, vals):\n    print(self.make_values(vals))\n</code></pre>"}]}