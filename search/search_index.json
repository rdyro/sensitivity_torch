{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Optimization Sensitivity in PyTorch - Reference Documentation - First- and Second-order Optimization","text":"<p><code>sensitivity_torch</code> is a package designed to allow taking first- and second-order derivatives through optimization or any other fixed-point process.</p> <p>Source code for this package is located here github.com/rdyro/sensitivity_torch</p> <p>This package builds on top of PyTorch. We also maintain an implementation in JAX here.</p> <p> </p>"},{"location":"installation/","title":"Installation","text":"<p>Install using pip <pre><code>$ pip install git+https://github.com/rdyro/sensitivity_torch.git\n</code></pre> or from source <pre><code>$ git clone git@github.com:rdyro/sensitivity_torch.git\n$ cd sensitivity_torch\n$ python3 setup.py install --user\n</code></pre></p>"},{"location":"installation/#testing","title":"Testing","text":"<p>Run all unit tests using <pre><code>$ python3 setup.py test\n</code></pre></p>"},{"location":"tour/","title":"Tour of <code>sensitivity_torch</code>","text":"<p>A simple, exhaustive, example of model tuning using sensitivity analysis <pre><code>import torch, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_moons\nfrom sensitivity_torch.sensitivity import (\nimplicit_jacobian,\ngenerate_optimization_fns,\n)\nfrom sensitivity_torch.extras.optimization import (\nminimize_agd,\nminimize_lbfgs,\nminimize_sqp,\n)\ndef feature_map(X, P, q):\nPhi = torch.cos(X @ P + q)\nreturn Phi\ndef optimize_fn(P, q, X, Y):\n\"\"\"Non-differentiable optimization function using sklearn.\"\"\"\nPhi = feature_map(X, P, q)\nmodel = LogisticRegression(max_iter=300, C=1e-2)\nmodel.fit(Phi, Y)\nz = np.concatenate([model.coef_.reshape(-1), model.intercept_])\nreturn torch.tensor(z)\ndef loss_fn(z, P, q, X, Y):\n\"\"\"The measure of model performance.\"\"\"\nPhi = feature_map(X, P, q)\nW, b = z[:-1], z[-1]\nprob = torch.sigmoid(Phi @ W + b)\nloss = torch.sum(-(Y * torch.log(prob) + (1 - Y) * torch.log(1 - prob)))\nregularization = 0.5 * torch.sum(W ** 2) / 1e-2\nreturn (loss + regularization) / X.shape[-2]\ndef k_fn(z, P, q, X, Y):\n\"\"\"Optimalty conditions of the model \u2013 the fixed-point of optimization.\"\"\"\nreturn torch.autograd.functional.jacobian(\nlambda z: loss_fn(z, P, q, X, Y), z, create_graph=True\n)\nif __name__ == \"__main__\":\n######################## SETUP ####################################\n# generate data and split into train and test sets\nX, Y = make_moons(n_samples=1000, noise=1e-1)\nX, Y = torch.tensor(X), torch.tensor(Y)\nn_train = 200\nXtr, Ytr = X[:n_train, :], Y[:n_train]\nXts, Yts = X[n_train:, :], Y[n_train:]\n# generate the Fourier feature map parameters Phi = cos(X @ P + q)\nn_features = 200\nP = torch.tensor(np.random.randn(2, n_features))\nq = torch.tensor(np.zeros(n_features))\n# check that the fixed-point is numerically close to zero\nz = optimize_fn(P, q, X, Y)\nk = k_fn(z, P, q, X, Y)\nassert torch.max(torch.abs(k)) &lt; 1e-5\n######################## SENSITIVITY ##############################\n# generate sensitivity Jacobians and check their shape\nJP, Jq = implicit_jacobian(lambda z, P, q: k_fn(z, P, q, X, Y), z, P, q)\nassert JP.shape == z.shape + P.shape\nassert Jq.shape == z.shape + q.shape\n######################## OPTIMIZATION #############################\n# generate necessary functions for optimization\nopt_fn_ = lambda P: optimize_fn(P, q, Xtr, Ytr)  # model optimization\nk_fn_ = lambda z, P: k_fn(z, P, q, Xtr, Ytr)  # fixed-point\nloss_fn_ = lambda z, P: loss_fn(z, P, q, Xts, Yts)  # loss to improve\nf_fn, g_fn, h_fn = generate_optimization_fns(loss_fn_, opt_fn_, k_fn_)\n# choose any optimization routine\n# Ps = minimize_agd(f_fn, g_fn, P, verbose=True, ai=1e-1, af=1e-1, max_it=100)\n# Ps = minimize_lbfgs(f_fn, g_fn, P, verbose=True, lr=1e-1, max_it=10)\nPs = minimize_sqp(f_fn, g_fn, h_fn, P, verbose=True, max_it=10)\ndef predict(z, P, q, X):\nPhi = feature_map(X, P, q)\nW, b = z[:-1], z[-1]\nprob = torch.sigmoid(Phi @ W + b)\nreturn torch.round(prob)\n# evaluate the results\nacc0 = torch.mean(1.0 * (predict(opt_fn_(P), P, q, Xts) == Yts))\naccf = torch.mean(1.0 * (predict(opt_fn_(Ps), Ps, q, Xts) == Yts))\nprint(\"Accuracy before: %4.2f%%\" % (1e2 * acc0))\nprint(\"Accuracy after:  %4.2f%%\" % (1e2 * accf))\nprint(\"Loss before:     %9.4e\" % loss_fn(opt_fn_(P), P, q, Xts, Yts))\nprint(\"Loss after:      %9.4e\" % loss_fn(opt_fn_(Ps), Ps, q, Xts, Yts))\n</code></pre></p>"},{"location":"api/overview/","title":"<code>batch_sensitivity</code>","text":"name summary generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad, optimizations) Directly generates upper/outer bilevel program derivative functions. implicit_hessian(k_fn, z, params, Dg, Hg, jvp_vec, optimizations) Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec. implicit_jacobian(k_fn, z, params, Dg, jvp_vec, matrix_free_inverse, full_output, optimizations) Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec."},{"location":"api/overview/#differentiation","title":"<code>differentiation</code>","text":"name summary BATCH_HESSIAN(fn, args, config) Computes the Hessian, assuming the first in/out dimension is the batch. BATCH_HESSIAN_DIAG(fn, args, config) Evaluates per-argument partial batch (first dimension) Hessians. BATCH_JACOBIAN(fn, args, config) Computes the Hessian, assuming the first in/out dimension is the batch. HESSIAN(fn, inputs, config) Equivalent to torch.autograd.functional.hessian HESSIAN_DIAG(fn, args, config) Generates a function which computes per-argument partial Hessians. JACOBIAN(fn, inputs, config) Equivalent to torch.autograd.functional.jacobian"},{"location":"api/overview/#extrasoptimization","title":"<code>extras.optimization</code>","text":"name summary minimize_agd(f_fn, g_fn, args, verbose, verbose_prefix, max_it, ai, af, batched, full_output, callback_fn, use_writer, use_tqdm, optimizer, optimizer_state, optimizer_opts) Minimize a loss function <code>f_fn</code> with Accelerated Gradient Descent (AGD) with respect to <code>*args</code>. Uses PyTorch. minimize_lbfgs(f_fn, g_fn, args, verbose, verbose_prefix, lr, max_it, batched, full_output, callback_fn, use_writer, use_tqdm) Minimize a loss function <code>f_fn</code> with L-BFGS with respect to <code>*args</code>. Taken from PyTorch. minimize_sqp(f_fn, g_fn, h_fn, args, reg0, verbose, verbose_prefix, max_it, ls_pts_nb, force_step, batched, full_output, callback_fn, use_writer, use_tqdm) Minimize a loss function <code>f_fn</code> with Unconstrained Sequential Quadratic Programming (SQP) with respect to a single <code>arg</code>."},{"location":"api/overview/#sensitivity","title":"<code>sensitivity</code>","text":"name summary generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad, optimizations) Directly generates upper/outer bilevel program derivative functions. implicit_hessian(k_fn, z, params, Dg, Hg, jvp_vec, optimizations) Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec. implicit_jacobian(k_fn, z, params, Dg, jvp_vec, matrix_free_inverse, full_output, optimizations) Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec."},{"location":"api/sensitivity_torch/batch_sensitivity/generate_optimization_fns/","title":"Generate optimization fns","text":"next &gt;&gt;&gt;<p>sensitivity_torch.implicit_hessian</p>"},{"location":"api/sensitivity_torch/batch_sensitivity/generate_optimization_fns/#sensitivity_torch.batch_sensitivity.generate_optimization_fns","title":"<code>sensitivity_torch.batch_sensitivity.generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> required <p>Returns:</p> Type Description <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code> parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_torch/batch_sensitivity.py</code> <pre><code>def generate_optimization_fns(\nloss_fn: Callable,\nopt_fn: Callable,\nk_fn: Callable,\nnormalize_grad: bool = False,\noptimizations: Mapping = None,\n):\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)`` parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\nsol_cache = dict()\nopt_fn_ = lambda *args, **kwargs: opt_fn(*args, **kwargs).detach()\noptimizations = {} if optimizations is None else copy(optimizations)\n@fn_with_sol_cache(opt_fn_, sol_cache)\ndef f_fn(z, *params):\nz = z.detach() if isinstance(z, Tensor) else z\nparams = _detach_args(*params)\nreturn loss_fn(z, *params)\n@fn_with_sol_cache(opt_fn_, sol_cache)\ndef g_fn(z, *params):\nz = z.detach() if isinstance(z, Tensor) else z\nparams = _detach_args(*params)\ng = JACOBIAN(loss_fn, (z, *params))\nDp = implicit_jacobian(k_fn, z.detach(), *params, Dg=g[0], optimizations=optimizations)\nDp = Dp if len(params) != 1 else [Dp]\nret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\nif normalize_grad:\nret = [(z / (torch.norm(z) + 1e-7)).detach() for z in ret]\nret = [ret.detach() for ret in ret]\nreturn ret[0] if len(ret) == 1 else ret\n@fn_with_sol_cache(opt_fn_, sol_cache)\ndef h_fn(z, *params):\nz = z.detach() if isinstance(z, Tensor) else z\nparams = _detach_args(*params)\ng = JACOBIAN(loss_fn, (z, *params))\nif optimizations.get(\"Hz_fn\", None) is None:\noptimizations[\"Hz_fn\"] = lambda z, *params: BATCH_HESSIAN_DIAG(\nlambda z: loss_fn(z, *params), (z,)\n)[0]\nHz_fn = optimizations[\"Hz_fn\"]\nHz = Hz_fn(z, *params)\nH = [Hz] + BATCH_HESSIAN_DIAG(lambda *params: loss_fn(z, *params), params)\nDp, Dpp = implicit_hessian(\nk_fn,\nz,\n*params,\nDg=g[0],\nHg=H[0],\noptimizations=optimizations,\n)\nDpp = Dpp if len(params) != 1 else [Dpp]\nret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\nret = [ret.detach() for ret in ret]\nreturn ret[0] if len(ret) == 1 else ret\nreturn f_fn, g_fn, h_fn\n</code></pre>"},{"location":"api/sensitivity_torch/batch_sensitivity/implicit_hessian/","title":"Implicit hessian","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.generate_optimization_fns</p>next &gt;&gt;&gt;<p>sensitivity_torch.implicit_jacobian</p>"},{"location":"api/sensitivity_torch/batch_sensitivity/implicit_hessian/#sensitivity_torch.batch_sensitivity.implicit_hessian","title":"<code>sensitivity_torch.batch_sensitivity.implicit_hessian(k_fn, z, *params, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>Tensor</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_torch/batch_sensitivity.py</code> <pre><code>def implicit_hessian(\nk_fn: Callable,\nz: Tensor,\n*params: Tensor,\nDg: Tensor = None,\nHg: Tensor = None,\njvp_vec: Union[Tensor, Sequence[Tensor]] = None,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nblen, zlen = z.shape[0], _prod(z.shape[1:])\nplen = [_prod(param.shape[1:]) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\nif jvp_vec is not None:\nassert Dg is not None\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn)\n#############################################################\n# compute 2nd implicit gradients\nif Dg is not None:\nassert Dg.numel() == blen * zlen\nassert Hg is None or Hg.numel() == blen * zlen**2\nDg_ = Dg.reshape((blen, zlen, 1))\nHg_ = Hg.reshape((blen, zlen, zlen)) if Hg is not None else Hg\n# compute the left hand vector in the VJP\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg_.reshape((blen, zlen, 1)), T=True)\nv = v.detach()\nfn = lambda z, *params: torch.sum(\nv.reshape((blen, zlen)) * k_fn(z, *params).reshape((blen, zlen))\n)\nif jvp_vec is not None:\nDpz_jvp = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\njvp_vec=jvp_vec,\noptimizations=optimizations,\n)\n)\nDpz_jvp = [Dpz_jvp.reshape((blen, -1)).detach() for Dpz_jvp in Dpz_jvp]\n# compute the 2nd order derivatives consisting of 4 terms\n# term 1 ##############################\nDpp1 = [\ntorch.autograd.functional.jvp(\nlambda param: JACOBIAN(\nlambda param: fn(z, *params[:i], param, *params[i + 1 :]),\nparam,\ncreate_graph=True,\n),\nparam,\njvp_vec[i],\n)[1].reshape((blen, -1))\nfor (i, param) in enumerate(params)\n]\n# term 2 ##############################\nDpp2 = [\ntorch.autograd.functional.jvp(\nlambda z: BATCH_JACOBIAN(\nlambda param: fn(z, *params[:i], param, *params[i + 1 :]),\nparams[i],\ncreate_graph=True,\n),\nz,\nDpz_jvp.reshape(z.shape),\n)[1].reshape((blen, -1))\nfor (i, (plen, Dpz_jvp)) in enumerate(zip(plen, Dpz_jvp))\n]\n# term 3 ##############################\ng_ = _ensure_list(\ntorch.autograd.functional.jvp(\nlambda *params: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\nparams,\ntuple(jvp_vec),\n)[1]\n)\nDpp3 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nDg=g_,\noptimizations=optimizations,\n)\n)[i].reshape((blen, -1))\nfor (i, g_) in enumerate(g_)\n]\n# term 4 ##############################\ng_ = [\ntorch.autograd.functional.jvp(\nlambda z: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\nz,\nDpz_jvp.reshape(z.shape),\n)[1]\nfor Dpz_jvp in Dpz_jvp\n]\nif Hg is not None:\ng_ = [\ng_.reshape((blen, zlen, 1)) + Hg_ @ Dpz_jvp.reshape((blen, zlen, 1))\nfor (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n]\nDpp4 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nDg=g_,\noptimizations=optimizations,\n)\n)[i].reshape((blen, -1))\nfor ((i, g_), plen) in zip(enumerate(g_), plen)\n]\nDp = [\nDg_.reshape((blen, 1, zlen)) @ Dpz_jvp.reshape((blen, zlen, 1))\nfor Dpz_jvp in Dpz_jvp\n]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape((blen,)) for Dp in Dp]\nDpp_shaped = [Dpp.reshape(param.shape) for (Dpp, param) in zip(Dpp, params)]\nelse:\n# compute the full first order 1st gradients\nDpz = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\noptimizations=optimizations,\n)\n)\nDpz = [Dpz.reshape((blen, zlen, plen)).detach() for (Dpz, plen) in zip(Dpz, plen)]\n# compute the 2nd order derivatives consisting of 4 terms\nDpp1 = BATCH_HESSIAN_DIAG(lambda *params: fn(z, *params), params)\nDpp1 = [Dpp1.reshape((blen, plen, plen)) for (Dpp1, plen) in zip(Dpp1, plen)]\ntemp = BATCH_JACOBIAN(\nlambda *params: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\nparams,\n)\nDpp2 = [\n(t(temp.reshape((blen, zlen, plen))) @ Dpz).reshape((blen, plen, plen))\nfor (temp, Dpz, plen) in zip(temp, Dpz, plen)\n]\nDpp3 = [t(Dpp2) for Dpp2 in Dpp2]\nDzz = BATCH_HESSIAN(lambda z: fn(z, *params), z).reshape((blen, zlen, zlen))\nif Hg is not None:\nDpp4 = [t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\nelse:\nDpp4 = [t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\nDp = [Dg_.reshape((blen, 1, zlen)) @ Dpz for Dpz in Dpz]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nDpp_shaped = [\nDpp.reshape((blen,) + 2 * param.shape[1:]) for (Dpp, param) in zip(Dpp, params)\n]\nreturn (Dp_shaped[0], Dpp_shaped[0]) if len(params) == 1 else (Dp_shaped, Dpp_shaped)\nelse:\nDpz, optimizations = implicit_jacobian(\nk_fn,\nz,\n*params,\nfull_output=True,\noptimizations=optimizations,\n)\nDpz = _ensure_list(Dpz)\nDpz = [Dpz.reshape((blen, zlen, plen)) for (Dpz, plen) in zip(Dpz, plen)]\n# compute derivatives\nif optimizations.get(\"Dzzk\", None) is None:\nHk = BATCH_HESSIAN_DIAG(k_fn, (z, *params))\nDzzk, Dppk = Hk[0], Hk[1:]\noptimizations[\"Dzzk\"] = Dzzk\nelse:\nDppk = BATCH_HESSIAN_DIAG(lambda *params: k_fn(z, *params), params)\nDppk = [Dppk.reshape((blen, zlen, plen, plen)) for (Dppk, plen) in zip(Dppk, plen)]\nDzpk = BATCH_JACOBIAN(\nlambda *params: BATCH_JACOBIAN(lambda z: k_fn(z, *params), z, create_graph=True),\nparams,\n)\nDzzk = Dzzk.reshape((blen, zlen, zlen, zlen))\nDzpk = [Dzpk.reshape((blen, zlen, zlen, plen)) for (Dzpk, plen) in zip(Dzpk, plen)]\nDpzk = [t(Dzpk) for Dzpk in Dzpk]\n# solve the IFT equation\nlhs = [\nDppk\n+ Dpzk @ Dpz[:, None, ...]\n+ t(Dpz)[:, None, ...] @ Dzpk\n+ (t(Dpz)[:, None, ...] @ Dzzk) @ Dpz[:, None, ...]\nfor (Dpz, Dzpk, Dpzk, Dppk) in zip(Dpz, Dzpk, Dpzk, Dppk)\n]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDppz = [\n-Dzk_solve_fn(z, *params, rhs=lhs.reshape((blen, zlen, plen * plen)), T=False).reshape(\n(blen, zlen, plen, plen)\n)\nfor (lhs, plen) in zip(lhs, plen)\n]\n# return computed values\nDpz_shaped = [\nDpz.reshape((blen,) + z.shape[1:] + param.shape[1:])\nfor (Dpz, param) in zip(Dpz, params)\n]\nDppz_shaped = [\nDppz.reshape((blen,) + z.shape[1:] + 2 * param.shape[1:])\nfor (Dppz, param) in zip(Dppz, params)\n]\nreturn (Dpz_shaped[0], Dppz_shaped[0]) if len(params) == 1 else (Dpz_shaped, Dppz_shaped)\n</code></pre>"},{"location":"api/sensitivity_torch/batch_sensitivity/implicit_jacobian/","title":"Implicit jacobian","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.implicit_hessian</p>next &gt;&gt;&gt;<p>sensitivity_torch.BATCH_HESSIAN</p>"},{"location":"api/sensitivity_torch/batch_sensitivity/implicit_jacobian/#sensitivity_torch.batch_sensitivity.implicit_jacobian","title":"<code>sensitivity_torch.batch_sensitivity.implicit_jacobian(k_fn, z, *params, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_torch/batch_sensitivity.py</code> <pre><code>def implicit_jacobian(\nk_fn: Callable,\nz: Tensor,\n*params: Tensor,\nDg: Tensor = None,\njvp_vec: Union[Tensor, Sequence[Tensor]] = None,\nmatrix_free_inverse: bool = False,\nfull_output: bool = False,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nblen, zlen = z.shape[0], _prod(z.shape[1:])\nplen = [_prod(param.shape[1:]) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn)\n#############################################################\nif Dg is not None:\nif matrix_free_inverse:\nraise NotImplementedError\nelse:\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg.reshape((blen, zlen, 1)), T=True)\nv = v.detach()\nfn = lambda *params: torch.sum(\nv.reshape((blen, zlen)) * k_fn(z, *params).reshape((blen, zlen))\n)\nDp = JACOBIAN(fn, params)\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nret = Dp_shaped[0] if len(params) == 1 else Dp_shaped\nelse:\nif jvp_vec is not None:\nDp = _ensure_list(\ntorch.autograd.functional.jvp(\nlambda *params: k_fn(z, *params),\ntuple(params),\ntuple(jvp_vec),\n)[1]\n)\nDp = [Dp.reshape((blen, zlen, 1)) for (Dp, plen) in zip(Dp, plen)]\nDpk = Dp\nelse:\nDpk = _ensure_list(BATCH_JACOBIAN(lambda *params: k_fn(z, *params), params))\nDpk = [Dpk.reshape((blen, zlen, plen)) for (Dpk, plen) in zip(Dpk, plen)]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDpz = [-Dzk_solve_fn(z, *params, rhs=Dpk, T=False) for Dpk in Dpk]\nif jvp_vec is not None:\nDpz_shaped = [Dpz.reshape(z.shape) for Dpz in Dpz]\nelse:\nDpz_shaped = [\nDpz.reshape((blen,) + z.shape[1:] + param.shape[1:])\nfor (Dpz, param) in zip(Dpz, params)\n]\nret = Dpz_shaped if len(params) != 1 else Dpz_shaped[0]\nreturn (ret, optimizations) if full_output else ret\n</code></pre>"},{"location":"api/sensitivity_torch/differentiation/BATCH_HESSIAN/","title":"BATCH HESSIAN","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.implicit_jacobian</p>next &gt;&gt;&gt;<p>sensitivity_torch.BATCH_HESSIAN_DIAG</p>"},{"location":"api/sensitivity_torch/differentiation/BATCH_HESSIAN/#sensitivity_torch.differentiation.BATCH_HESSIAN","title":"<code>sensitivity_torch.differentiation.BATCH_HESSIAN(fn, args, **config)</code>","text":"<p>Computes the Hessian, assuming the first in/out dimension is the batch.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def BATCH_HESSIAN(fn, args, **config):\n\"\"\"Computes the Hessian, assuming the first in/out dimension is the batch.\"\"\"\nsingle_input = not isinstance(args, (list, tuple))\nargs = (args,) if single_input else tuple(args)\nassert single_input\nreturn BATCH_JACOBIAN(\nlambda *args: BATCH_JACOBIAN(fn, args, **dict(config, create_graph=True))[0],\nargs,\n**config,\n)[0]\n</code></pre>"},{"location":"api/sensitivity_torch/differentiation/BATCH_HESSIAN_DIAG/","title":"BATCH HESSIAN DIAG","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.BATCH_HESSIAN</p>next &gt;&gt;&gt;<p>sensitivity_torch.BATCH_JACOBIAN</p>"},{"location":"api/sensitivity_torch/differentiation/BATCH_HESSIAN_DIAG/#sensitivity_torch.differentiation.BATCH_HESSIAN_DIAG","title":"<code>sensitivity_torch.differentiation.BATCH_HESSIAN_DIAG(fn, args, **config)</code>","text":"<p>Evaluates per-argument partial batch (first dimension) Hessians.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def BATCH_HESSIAN_DIAG(fn, args, **config):\n\"\"\"Evaluates per-argument partial batch (first dimension) Hessians.\"\"\"\nsingle_input = not isinstance(args, (list, tuple))\nargs = (args,) if single_input else tuple(args)\ntry:\nret = [\nBATCH_HESSIAN(lambda arg: fn(*args[:i], arg, *args[i + 1 :]), arg, **config)\nfor (i, arg) in enumerate(args)\n]\nexcept RuntimeError:  # function has more than 1 output, need to use JACOBIAN\nassert False\nret = [\nBATCH_JACOBIAN(\nlambda arg: BATCH_JACOBIAN(\nlambda arg: fn(*args[:i], arg, *args[i + 1 :]),\narg,\n**dict(config, create_graph=True),\n),\narg,\n**config,\n)\nfor (i, arg) in enumerate(args)\n]\nreturn ret[0] if single_input else ret\n</code></pre>"},{"location":"api/sensitivity_torch/differentiation/BATCH_JACOBIAN/","title":"BATCH JACOBIAN","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.BATCH_HESSIAN_DIAG</p>next &gt;&gt;&gt;<p>sensitivity_torch.HESSIAN</p>"},{"location":"api/sensitivity_torch/differentiation/BATCH_JACOBIAN/#sensitivity_torch.differentiation.BATCH_JACOBIAN","title":"<code>sensitivity_torch.differentiation.BATCH_JACOBIAN(fn, args, **config)</code>","text":"<p>Computes the Hessian, assuming the first in/out dimension is the batch.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def BATCH_JACOBIAN(fn, args, **config):\n\"\"\"Computes the Hessian, assuming the first in/out dimension is the batch.\"\"\"\nsingle_input = not isinstance(args, (list, tuple))\nargs = (args,) if single_input else tuple(args)\nJs = JACOBIAN(lambda *args: torch.sum(fn(*args), 0), args, **config)\nout_shapes = [J.shape[: -len(arg.shape)] for (J, arg) in zip(Js, args)]\nJs = [\nJ.reshape((prod(out_shape),) + arg.shape)\n.swapaxes(0, 1)\n.reshape((arg.shape[0],) + out_shape + arg.shape[1:])\nfor (J, out_shape, arg) in zip(Js, out_shapes, args)\n]\nreturn Js[0] if single_input else Js\n</code></pre>"},{"location":"api/sensitivity_torch/differentiation/HESSIAN/","title":"HESSIAN","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.BATCH_JACOBIAN</p>next &gt;&gt;&gt;<p>sensitivity_torch.HESSIAN_DIAG</p>"},{"location":"api/sensitivity_torch/differentiation/HESSIAN/#sensitivity_torch.differentiation.HESSIAN","title":"<code>sensitivity_torch.differentiation.HESSIAN(fn, inputs, **config)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def HESSIAN(fn, inputs, **config):\nconfig.setdefault(\"vectorize\", True)\nreturn torch.autograd.functional.hessian(fn, inputs, **config)\n</code></pre>"},{"location":"api/sensitivity_torch/differentiation/HESSIAN_DIAG/","title":"HESSIAN DIAG","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.HESSIAN</p>next &gt;&gt;&gt;<p>sensitivity_torch.JACOBIAN</p>"},{"location":"api/sensitivity_torch/differentiation/HESSIAN_DIAG/#sensitivity_torch.differentiation.HESSIAN_DIAG","title":"<code>sensitivity_torch.differentiation.HESSIAN_DIAG(fn, args, **config)</code>","text":"<p>Generates a function which computes per-argument partial Hessians.</p> Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def HESSIAN_DIAG(fn, args, **config):\n\"\"\"Generates a function which computes per-argument partial Hessians.\"\"\"\nsingle_input = not isinstance(args, (list, tuple))\nargs = (args,) if single_input else tuple(args)\ntry:\nret = [\nHESSIAN(lambda arg: fn(*args[:i], arg, *args[i + 1 :]), arg, **config)\nfor (i, arg) in enumerate(args)\n]\nexcept RuntimeError:  # function has more than 1 output, need to use JACOBIAN\nret = [\nJACOBIAN(\nlambda arg: JACOBIAN(\nlambda arg: fn(*args[:i], arg, *args[i + 1 :]),\narg,\n**dict(config, create_graph=True),\n),\narg,\n**config,\n)\nfor (i, arg) in enumerate(args)\n]\nreturn ret[0] if single_input else ret\n</code></pre>"},{"location":"api/sensitivity_torch/differentiation/JACOBIAN/","title":"JACOBIAN","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.HESSIAN_DIAG</p>next &gt;&gt;&gt;<p>sensitivity_torch.extras.minimize_agd</p>"},{"location":"api/sensitivity_torch/differentiation/JACOBIAN/#sensitivity_torch.differentiation.JACOBIAN","title":"<code>sensitivity_torch.differentiation.JACOBIAN(fn, inputs, **config)</code>","text":"Source code in <code>sensitivity_torch/differentiation.py</code> <pre><code>def JACOBIAN(fn, inputs, **config):\nconfig.setdefault(\"vectorize\", True)\nreturn torch.autograd.functional.jacobian(fn, inputs, **config)\n</code></pre>"},{"location":"api/sensitivity_torch/extras/optimization/minimize_agd/","title":"Minimize agd","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.JACOBIAN</p>next &gt;&gt;&gt;<p>sensitivity_torch.extras.minimize_lbfgs</p>"},{"location":"api/sensitivity_torch/extras/optimization/minimize_agd/#sensitivity_torch.extras.optimization.minimize_agd","title":"<code>sensitivity_torch.extras.optimization.minimize_agd(f_fn, g_fn, *args, verbose=False, verbose_prefix='', max_it=10 ** 3, ai=0.1, af=0.01, batched=False, full_output=False, callback_fn=None, use_writer=False, use_tqdm=True, optimizer='adam', optimizer_state=None, optimizer_opts=None)</code>","text":"<p>Minimize a loss function <code>f_fn</code> with Accelerated Gradient Descent (AGD) with respect to <code>*args</code>. Uses PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>f_fn</code> <code>Callable</code> <p>loss function</p> required <code>g_fn</code> <code>Callable</code> <p>gradient of the loss function</p> required <code>*args</code> <code>Tensor</code> <p>arguments to be optimized</p> <code>()</code> <code>verbose</code> <code>bool</code> <p>whether to print output</p> <code>False</code> <code>verbose_prefix</code> <code>str</code> <p>prefix to append to verbose output, e.g. indentation</p> <code>''</code> <code>max_it</code> <code>int</code> <p>maximum number of iterates</p> <code>10 ** 3</code> <code>ai</code> <code>float</code> <p>initial gradient step length (exponential schedule)</p> <code>0.1</code> <code>af</code> <code>float</code> <p>final gradient step length (exponential schedule)</p> <code>0.01</code> <code>batched</code> <code>bool</code> <p>whether to optimize a batch of arguments, with batch of losses</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to output optimization history</p> <code>False</code> <code>callback_fn</code> <code>Callable</code> <p>callback function of the form <code>cb_fn(*args, **kw)</code></p> <code>None</code> <code>use_writer</code> <code>bool</code> <p>whether to use tensorflow's Summary Writer (via PyTorch)</p> <code>False</code> <code>use_tqdm</code> <code>Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook]</code> <p>whether to use tqdm (to estimate total runtime)</p> <code>True</code> <p>Returns:</p> Type Description <p>Optimized <code>args</code> or <code>(args, args_hist)</code> if <code>full_output</code> is <code>True</code></p> Source code in <code>sensitivity_torch/extras/optimization.py</code> <pre><code>def minimize_agd(\nf_fn: Callable,\ng_fn: Callable,\n*args: Tensor,\nverbose: bool = False,\nverbose_prefix: str = \"\",\nmax_it: int = 10**3,\nai: float = 1e-1,\naf: float = 1e-2,\nbatched: bool = False,\nfull_output: bool = False,\ncallback_fn: Callable = None,\nuse_writer: bool = False,\nuse_tqdm: Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook] = True,\noptimizer: str = \"adam\",\noptimizer_state: Mapping = None,\noptimizer_opts: Mapping = None,\n):\n\"\"\"Minimize a loss function ``f_fn`` with Accelerated Gradient Descent (AGD)\n    with respect to ``*args``. Uses PyTorch.\n    Args:\n        f_fn: loss function\n        g_fn: gradient of the loss function\n        *args: arguments to be optimized\n        verbose: whether to print output\n        verbose_prefix: prefix to append to verbose output, e.g. indentation\n        max_it: maximum number of iterates\n        ai: initial gradient step length (exponential schedule)\n        af: final gradient step length (exponential schedule)\n        batched: whether to optimize a batch of arguments, with batch of losses\n        full_output: whether to output optimization history\n        callback_fn: callback function of the form ``cb_fn(*args, **kw)``\n        use_writer: whether to use tensorflow's Summary Writer (via PyTorch)\n        use_tqdm: whether to use tqdm (to estimate total runtime)\n    Returns:\n        Optimized ``args`` or ``(args, args_hist)`` if ``full_output`` is ``True``\n    \"\"\"\nif isinstance(use_tqdm, bool):\nif use_tqdm:\nprint_fn, rng_wrapper = tqdm_module.tqdm.write, tqdm_module.tqdm\nelse:\nprint_fn, rng_wrapper = print, lambda x: x\nelse:\nprint_fn, rng_wrapper = use_tqdm.write, use_tqdm\nassert len(args) &gt; 0\nassert g_fn is not None or all([isinstance(arg, torch.Tensor) for arg in args])\nargs = [arg.clone().detach() for arg in args]\nfor arg in args:\narg.requires_grad = True\nimprv = float(\"inf\")\ngam = (af / ai) ** (1.0 / max_it)\n# make the optimizer\noptimizer_opts = {} if optimizer_opts is None else optimizer_opts\nif optimizer.lower() == \"adam\":\nopt = torch.optim.Adam(args, lr=ai, **optimizer_opts)\nelif optimizer.lower() == \"sgd\":\nopt = torch.optim.SGD(args, lr=ai, **optimizer_opts)\nelif optimizer.lower() == \"rmsprop\":\nopt = torch.optim.RMSprop(args, lr=ai, **optimizer_opts)\nif optimizer_state is not None:\nopt.load_state_dict(optimizer_state)\ntp = TablePrinter(\n[\"it\", \"imprv\", \"loss\", \"||g||_2\"],\n[\"%05d\", \"%9.4e\", \"%9.4e\", \"%9.4e\"],\nprefix=verbose_prefix,\nuse_writer=use_writer,\n)\n# args_hist = [[arg.detach().clone() for arg in args]]\nargs_hist, grads_hist = [], []\nif callback_fn is not None:\ncallback_fn(*args, step=-1, optimizer=opt)\nif verbose:\nprint_fn(tp.make_header())\nfor it in rng_wrapper(range(max_it)):\nargs_prev = [arg.clone().detach() for arg in args]\nopt.zero_grad()\nif g_fn is None:\nloss = torch.sum(f_fn(*args))\nloss.backward()\nif batched:\nloss = loss / args[0].shape[0]\nelse:\nargs_ = [arg for arg in args]\nloss = torch.mean(f_fn(*args_))\ngs = g_fn(*args_)\ngs = gs if isinstance(gs, list) or isinstance(gs, tuple) else [gs]\nfor arg, g in zip(args, gs):\narg.grad = torch.detach(g)\ng_norm = sum(torch.norm(arg.grad) for arg in args if arg.grad is not None).detach() / len(\nargs\n)\nopt.step()\nif full_output:\nargs_hist.append([arg.detach().clone() for arg in args])\ngrads_hist.append(\n[arg.grad.detach().clone() if arg.grad is not None else None for arg in args]\n)\nif callback_fn is not None:\ncallback_fn(*args, step=it, optimizer=opt)\nif batched:\nimprv = sum(\ntorch.mean(torch.norm(arg_prev - arg, dim=tuple(range(-(arg.ndim - 1), 0))))\nfor (arg, arg_prev) in zip(args, args_prev)\n)\nelse:\nimprv = sum(torch.norm(arg_prev - arg) for (arg, arg_prev) in zip(args, args_prev))\nif verbose or use_writer:\nvalues = tp.make_values([it, imprv.detach(), loss.detach(), g_norm])\nif verbose:\nprint_fn(values)\nfor pgroup in opt.param_groups:\npgroup[\"lr\"] *= gam\nit += 1\nif verbose:\nprint_fn(tp.make_footer())\nret = [arg.detach() for arg in args]\nret = ret if len(args) &gt; 1 else ret[0]\nargs_hist = [z if len(args) &gt; 1 else z[0] for z in args_hist]\ngrads_hist = [z if z is None or len(args) &gt; 1 else z[0] for z in grads_hist]\nif full_output:\nreturn ret, args_hist, grads_hist\nelse:\nreturn ret\n</code></pre>"},{"location":"api/sensitivity_torch/extras/optimization/minimize_lbfgs/","title":"Minimize lbfgs","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.extras.minimize_agd</p>next &gt;&gt;&gt;<p>sensitivity_torch.extras.minimize_sqp</p>"},{"location":"api/sensitivity_torch/extras/optimization/minimize_lbfgs/#sensitivity_torch.extras.optimization.minimize_lbfgs","title":"<code>sensitivity_torch.extras.optimization.minimize_lbfgs(f_fn, g_fn, *args, verbose=False, verbose_prefix='', lr=1.0, max_it=100, batched=False, full_output=False, callback_fn=None, use_writer=False, use_tqdm=True)</code>","text":"<p>Minimize a loss function <code>f_fn</code> with L-BFGS with respect to <code>*args</code>. Taken from PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>f_fn</code> <code>Callable</code> <p>loss function</p> required <code>g_fn</code> <code>Callable</code> <p>gradient of the loss function</p> required <code>*args</code> <code>Tensor</code> <p>arguments to be optimized</p> <code>()</code> <code>verbose</code> <code>bool</code> <p>whether to print output</p> <code>False</code> <code>verbose_prefix</code> <code>str</code> <p>prefix to append to verbose output, e.g. indentation</p> <code>''</code> <code>lr</code> <code>float</code> <p>learning rate, where 1.0 is unstable, use 1e-1 in most cases</p> <code>1.0</code> <code>max_it</code> <code>int</code> <p>maximum number of iterates</p> <code>100</code> <code>batched</code> <code>bool</code> <p>whether to optimize a batch of arguments, with batch of losses</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to output optimization history</p> <code>False</code> <code>callback_fn</code> <code>Callable</code> <p>callback function of the form <code>cb_fn(*args, **kw)</code></p> <code>None</code> <code>use_writer</code> <code>bool</code> <p>whether to use tensorflow's Summary Writer (via PyTorch)</p> <code>False</code> <code>use_tqdm</code> <code>Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook]</code> <p>whether to use tqdm (to estimate total runtime)</p> <code>True</code> <p>Returns:</p> Type Description <p>Optimized <code>args</code> or <code>(args, args_hist)</code> if <code>full_output</code> is <code>True</code></p> Source code in <code>sensitivity_torch/extras/optimization.py</code> <pre><code>def minimize_lbfgs(\nf_fn: Callable,\ng_fn: Callable,\n*args: Tensor,\nverbose: bool = False,\nverbose_prefix: str = \"\",\nlr: float = 1e0,\nmax_it: int = 100,\nbatched: bool = False,\nfull_output: bool = False,\ncallback_fn: Callable = None,\nuse_writer: bool = False,\nuse_tqdm: Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook] = True,\n):\n\"\"\"Minimize a loss function ``f_fn`` with L-BFGS with respect to ``*args``.\n    Taken from PyTorch.\n    Args:\n        f_fn: loss function\n        g_fn: gradient of the loss function\n        *args: arguments to be optimized\n        verbose: whether to print output\n        verbose_prefix: prefix to append to verbose output, e.g. indentation\n        lr: learning rate, where 1.0 is unstable, use 1e-1 in most cases\n        max_it: maximum number of iterates\n        batched: whether to optimize a batch of arguments, with batch of losses\n        full_output: whether to output optimization history\n        callback_fn: callback function of the form ``cb_fn(*args, **kw)``\n        use_writer: whether to use tensorflow's Summary Writer (via PyTorch)\n        use_tqdm: whether to use tqdm (to estimate total runtime)\n    Returns:\n        Optimized ``args`` or ``(args, args_hist)`` if ``full_output`` is ``True``\n    \"\"\"\nif isinstance(use_tqdm, bool):\nif use_tqdm:\nprint_fn, rng_wrapper = tqdm_module.tqdm.write, tqdm_module.tqdm\nelse:\nprint_fn, rng_wrapper = print, lambda x: x\nelse:\nprint_fn, rng_wrapper = use_tqdm.write, use_tqdm\nassert len(args) &gt; 0\nassert g_fn is not None or all([isinstance(arg, torch.Tensor) for arg in args])\nargs = [arg.detach().clone() for arg in args]\nfor arg in args:\narg.requires_grad = True\nimprv = float(\"inf\")\nit = 0\nopt = torch.optim.LBFGS(args, lr=lr)\n# args_hist = [[arg.detach().clone() for arg in args]]\nargs_hist, grads_hist = [], []\nif callback_fn is not None:\ncallback_fn(*args)\ndef closure():\nopt.zero_grad()\nif g_fn is None:\nloss = torch.sum(f_fn(*args))\nloss.backward()\nif batched:\nloss = loss / args[0].shape[0]\nelse:\nargs_ = [arg for arg in args]\nloss = torch.mean(f_fn(*args_))\ngs = g_fn(*args_)\ngs = gs if isinstance(gs, list) or isinstance(gs, tuple) else [gs]\nfor arg, g in zip(args, gs):\narg.grad = torch.detach(g)\nreturn loss\ntp = TablePrinter(\n[\"it\", \"imprv\", \"loss\", \"||g||_2\"],\n[\"%05d\", \"%9.4e\", \"%9.4e\", \"%9.4e\"],\nprefix=verbose_prefix,\nuse_writer=use_writer,\n)\nif verbose:\nprint_fn(tp.make_header())\nfor it in rng_wrapper(range(max_it)):\nargs_prev = [arg.detach().clone() for arg in args]\nloss = opt.step(closure)\nif full_output:\nargs_hist.append([arg.detach().clone() for arg in args])\ngrads_hist.append(\n[arg.grad.detach().clone() if arg.grad is not None else None for arg in args]\n)\nif callback_fn is not None:\ncallback_fn(*args)\nif batched:\nimprv = sum(\ntorch.mean(torch.norm(arg_prev - arg, dim=tuple(range(-(arg.ndim - 1), 0))))\nfor (arg, arg_prev) in zip(args, args_prev)\n)\nelse:\nimprv = sum(\ntorch.norm(arg_prev - arg).detach() for (arg, arg_prev) in zip(args, args_prev)\n)\nif verbose:\nclosure()\ng_norm = sum(arg.grad.norm().detach() for arg in args if arg.grad is not None)\nprint_fn(tp.make_values([it, imprv.detach(), loss.detach(), g_norm]))\nif imprv &lt; 1e-9:\nbreak\nit += 1\nif verbose:\nprint_fn(tp.make_footer())\nret = [arg.detach() for arg in args]\nret = ret if len(args) &gt; 1 else ret[0]\nargs_hist = [z if len(args) &gt; 1 else z[0] for z in args_hist]\ngrads_hist = [z if z is None or len(args) &gt; 1 else z[0] for z in grads_hist]\nif full_output:\nreturn ret, args_hist, grads_hist\nelse:\nreturn ret\n</code></pre>"},{"location":"api/sensitivity_torch/extras/optimization/minimize_sqp/","title":"Minimize sqp","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.extras.minimize_lbfgs</p>next &gt;&gt;&gt;<p>sensitivity_torch.generate_optimization_fns</p>"},{"location":"api/sensitivity_torch/extras/optimization/minimize_sqp/#sensitivity_torch.extras.optimization.minimize_sqp","title":"<code>sensitivity_torch.extras.optimization.minimize_sqp(f_fn, g_fn, h_fn, *args, reg0=1e-07, verbose=False, verbose_prefix='', max_it=100, ls_pts_nb=5, force_step=False, batched=False, full_output=False, callback_fn=None, use_writer=False, use_tqdm=True)</code>","text":"<p>Minimize a loss function <code>f_fn</code> with Unconstrained Sequential Quadratic Programming (SQP) with respect to a single <code>arg</code>.</p> <p>Parameters:</p> Name Type Description Default <code>f_fn</code> <code>Callable</code> <p>loss function</p> required <code>g_fn</code> <code>Callable</code> <p>gradient of the loss function</p> required <code>h_fn</code> <code>Callable</code> <p>Hessian of the loss function</p> required <code>*args</code> <code>Tensor</code> <p>arguments to be optimized</p> <code>()</code> <code>reg0</code> <code>float</code> <p>Hessian regularization \u2013 optimization step regularization</p> <code>1e-07</code> <code>verbose</code> <code>bool</code> <p>whether to print output</p> <code>False</code> <code>verbose_prefix</code> <code>str</code> <p>prefix to append to verbose output, e.g. indentation</p> <code>''</code> <code>max_it</code> <code>int</code> <p>maximum number of iterates</p> <code>100</code> <code>batched</code> <code>bool</code> <p>whether to optimize a batch of arguments, with batch of losses</p> <code>False</code> <code>ls_pts_nb</code> <code>int</code> <p>number of linesearch points to consider per optimization step</p> <code>5</code> <code>force_step</code> <code>bool</code> <p>whether to take any non-zero optimization step even if worse</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to output optimization history</p> <code>False</code> <code>callback_fn</code> <code>Callable</code> <p>callback function of the form <code>cb_fn(*args, **kw)</code></p> <code>None</code> <code>use_writer</code> <code>bool</code> <p>whether to use tensorflow's Summary Writer (via PyTorch)</p> <code>False</code> <code>use_tqdm</code> <code>Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook]</code> <p>whether to use tqdm (to estimate total runtime)</p> <code>True</code> <p>Returns:</p> Type Description <p>Optimized <code>args</code> or <code>(args, args_hist)</code> if <code>full_output</code> is <code>True</code></p> Source code in <code>sensitivity_torch/extras/optimization.py</code> <pre><code>def minimize_sqp(\nf_fn: Callable,\ng_fn: Callable,\nh_fn: Callable,\n*args: Tensor,\nreg0: float = 1e-7,\nverbose: bool = False,\nverbose_prefix: str = \"\",\nmax_it: int = 100,\nls_pts_nb: int = 5,\nforce_step: bool = False,\nbatched: bool = False,\nfull_output: bool = False,\ncallback_fn: Callable = None,\nuse_writer: bool = False,\nuse_tqdm: Union[bool, tqdm_module.std.tqdm, tqdm_module.notebook.tqdm_notebook] = True,\n):\n\"\"\"Minimize a loss function ``f_fn`` with Unconstrained Sequential Quadratic\n    Programming (SQP) with respect to a single ``arg``.\n    Args:\n        f_fn: loss function\n        g_fn: gradient of the loss function\n        h_fn: Hessian of the loss function\n        *args: arguments to be optimized\n        reg0: Hessian regularization \u2013 optimization step regularization\n        verbose: whether to print output\n        verbose_prefix: prefix to append to verbose output, e.g. indentation\n        max_it: maximum number of iterates\n        batched: whether to optimize a batch of arguments, with batch of losses\n        ls_pts_nb: number of linesearch points to consider per optimization step\n        force_step: whether to take any non-zero optimization step even if worse\n        full_output: whether to output optimization history\n        callback_fn: callback function of the form ``cb_fn(*args, **kw)``\n        use_writer: whether to use tensorflow's Summary Writer (via PyTorch)\n        use_tqdm: whether to use tqdm (to estimate total runtime)\n    Returns:\n        Optimized ``args`` or ``(args, args_hist)`` if ``full_output`` is ``True``\n    \"\"\"\nif isinstance(use_tqdm, bool):\nif use_tqdm:\nprint_fn, rng_wrapper = tqdm_module.tqdm.write, tqdm_module.tqdm\nelse:\nprint_fn, rng_wrapper = print, lambda x: x\nelse:\nprint_fn, rng_wrapper = use_tqdm.write, use_tqdm\nif len(args) &gt; 1:\nraise ValueError(\"SQP only only supports single variable functions\")\nx = args[0]\nx_shape = x.shape\nif batched:\nM, x_size = x_shape[0], np.prod(x_shape[1:])\nelse:\nM, x_size = 1, x.numel()\nit, imprv = 0, float(\"inf\")\nx_best, f_best = x, torch.atleast_1d(f_fn(x))\nf_hist, x_hist = [f_best], [x.detach().clone()]\nif callback_fn is not None:\npdb.set_trace()\ncallback_fn(x)\ntp = TablePrinter(\n[\"it\", \"imprv\", \"loss\", \"reg_it\", \"bet\", \"||g_prev||_2\"],\n[\"%05d\", \"%9.4e\", \"%9.4e\", \"%02d\", \"%9.4e\", \"%9.4e\"],\nprefix=verbose_prefix,\nuse_writer=use_writer,\n)\nif verbose:\nprint_fn(tp.make_header())\nfor it in rng_wrapper(range(max_it)):\ng = g_fn(x).reshape((M, x_size))\nH = h_fn(x).reshape((M, x_size, x_size))\nif torch.any(torch.isnan(g)):\nraise RuntimeError(\"Gradient is NaN\")\nif torch.any(torch.isnan(H)):\nraise RuntimeError(\"Hessian is NaN\")\n# F, (reg_it_max, _) = _positive_factorization_cholesky(H, reg0)\nF, (reg_it_max, _) = _positive_factorization_lobpcg(H, reg0)\nd = torch.cholesky_solve(-g[..., None], F)[..., 0].reshape(x_shape)\n# F = H + reg0 * I\n# d = torch.solve(F, -g[..., None])[..., 0].reshape(x_shape)\nf = f_hist[-1]\nbet, data = _linesearch(\nf,\nx,\nd,\nf_fn,\ng_fn,\nls_pts_nb=ls_pts_nb,\nforce_step=force_step,\n)\nx = x + torch.reshape(bet, (M,) + (1,) * len(x_shape[1:])) * d\nx_hist.append(x.clone().detach())\nimprv = torch.mean(bet * data[\"d_norm\"]).detach()\nif callback_fn is not None:\ncallback_fn(x)\nif batched:\nx_bests = [None for _ in range(M)]\nf_bests = [None for _ in range(M)]\nfor i in range(M):\nif data[\"f_best\"][i] &lt; f_best[i]:\nx_bests[i], f_bests[i] = x[i, ...], data[\"f_best\"][i]\nelse:\nx_bests[i], f_bests[i] = x_best[i, ...], f_best[i]\nx_best, f_best = torch.stack(x_bests), torch.stack(f_bests)\nelse:\nif data[\"f_best\"][0] &lt; f_best[0]:\nx_best, f_best = x, data[\"f_best\"]\nf_hist.append(data[\"f_best\"])\nif verbose:\nprint_fn(\ntp.make_values(\n[\nit,\nimprv,\ntorch.mean(data[\"f_best\"]),\nreg_it_max,\nbet[0],\ntorch.norm(g),\n]\n)\n)\nif imprv &lt; 1e-9:\nbreak\nit += 1\nif verbose:\nprint_fn(tp.make_footer())\nif full_output:\nreturn x_best, x_hist + [x_best.detach().clone()]\nelse:\nreturn x_best\n</code></pre>"},{"location":"api/sensitivity_torch/sensitivity/generate_optimization_fns/","title":"Generate optimization fns","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.extras.minimize_sqp</p>next &gt;&gt;&gt;<p>sensitivity_torch.implicit_hessian</p>"},{"location":"api/sensitivity_torch/sensitivity/generate_optimization_fns/#sensitivity_torch.sensitivity.generate_optimization_fns","title":"<code>sensitivity_torch.sensitivity.generate_optimization_fns(loss_fn, opt_fn, k_fn, normalize_grad=False, optimizations=None)</code>","text":"<p>Directly generates upper/outer bilevel program derivative functions.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable</code> <p>loss_fn(z, *params), upper/outer level loss</p> required <code>opt_fn</code> <code>Callable</code> <p>opt_fn(*params) = z, lower/inner argmin function</p> required <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>normalize_grad</code> <code>bool</code> <p>whether to normalize the gradient by its norm</p> <code>False</code> <code>jit</code> <p>whether to apply just-in-time (jit) compilation to the functions</p> required <p>Returns:</p> Type Description <code>Tuple[Callable, Callable, Callable]</code> <p><code>f_fn(*params), g_fn(*params), h_fn(*params)</code> parameters-only upper/outer level loss, gradient and Hessian.</p> Source code in <code>sensitivity_torch/sensitivity.py</code> <pre><code>def generate_optimization_fns(\nloss_fn: Callable,\nopt_fn: Callable,\nk_fn: Callable,\nnormalize_grad: bool = False,\noptimizations: Mapping = None,\n) -&gt; Tuple[Callable, Callable, Callable]:\n\"\"\"Directly generates upper/outer bilevel program derivative functions.\n    Args:\n        loss_fn: loss_fn(z, *params), upper/outer level loss\n        opt_fn: opt_fn(*params) = z, lower/inner argmin function\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        normalize_grad: whether to normalize the gradient by its norm\n        jit: whether to apply just-in-time (jit) compilation to the functions\n    Returns:\n        ``f_fn(*params), g_fn(*params), h_fn(*params)`` parameters-only upper/outer level loss, gradient and Hessian.\n    \"\"\"\nsol_cache = dict()\nopt_fn_ = lambda *args, **kwargs: opt_fn(*args, **kwargs).detach()\noptimizations = {} if optimizations is None else copy(optimizations)\n@fn_with_sol_cache(opt_fn_, sol_cache)\ndef f_fn(z, *params):\nz = z.detach() if isinstance(z, torch.Tensor) else z\nparams = _detach_args(*params)\nreturn loss_fn(z, *params)\n@fn_with_sol_cache(opt_fn_, sol_cache)\ndef g_fn(z, *params):\nz = z.detach() if isinstance(z, torch.Tensor) else z\nparams = _detach_args(*params)\ng = JACOBIAN(loss_fn, (z, *params))\nDp = implicit_jacobian(k_fn, z.detach(), *params, Dg=g[0], optimizations=optimizations)\nDp = Dp if len(params) != 1 else [Dp]\n# opts = dict(device=z.device, dtype=z.dtype)\n# Dp = [\n#    torch.zeros(param.shape, **opts) for param in params\n# ]\nret = [Dp + g for (Dp, g) in zip(Dp, g[1:])]\nif normalize_grad:\nret = [(z / (torch.norm(z) + 1e-7)).detach() for z in ret]\nret = [ret.detach() for ret in ret]\nreturn ret[0] if len(ret) == 1 else ret\n@fn_with_sol_cache(opt_fn_, sol_cache)\ndef h_fn(z, *params):\nz = z.detach() if isinstance(z, torch.Tensor) else z\nparams = _detach_args(*params)\ng = JACOBIAN(loss_fn, (z, *params))\nif optimizations.get(\"Hz_fn\", None) is None:\noptimizations[\"Hz_fn\"] = lambda z, *params: HESSIAN_DIAG(\nlambda z: loss_fn(z, *params), (z,)\n)[0]\nHz_fn = optimizations[\"Hz_fn\"]\nHz = Hz_fn(z, *params)\nH = [Hz] + HESSIAN_DIAG(lambda *params: loss_fn(z, *params), params)\nDp, Dpp = implicit_hessian(\nk_fn,\nz,\n*params,\nDg=g[0],\nHg=H[0],\noptimizations=optimizations,\n)\nDpp = Dpp if len(params) != 1 else [Dpp]\nret = [Dpp + H for (Dpp, H) in zip(Dpp, H[1:])]\nret = [ret.detach() for ret in ret]\nreturn ret[0] if len(ret) == 1 else ret\nreturn f_fn, g_fn, h_fn\n</code></pre>"},{"location":"api/sensitivity_torch/sensitivity/implicit_hessian/","title":"Implicit hessian","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.generate_optimization_fns</p>next &gt;&gt;&gt;<p>sensitivity_torch.implicit_jacobian</p>"},{"location":"api/sensitivity_torch/sensitivity/implicit_hessian/#sensitivity_torch.sensitivity.implicit_hessian","title":"<code>sensitivity_torch.sensitivity.implicit_hessian(k_fn, z, *params, Dg=None, Hg=None, jvp_vec=None, optimizations=None)</code>","text":"<p>Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>gradient sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>Hg</code> <code>Tensor</code> <p>Hessian sensitivity vector (wrt z), for chain rule</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for Hessian-vector-product</p> <code>None</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Hessian/chain rule Hessian as specified by arguments</p> Source code in <code>sensitivity_torch/sensitivity.py</code> <pre><code>def implicit_hessian(\nk_fn: Callable,\nz: Tensor,\n*params: Tensor,\nDg: Tensor = None,\nHg: Tensor = None,\njvp_vec: Union[Tensor, Sequence[Tensor]] = None,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Hessian or chain rule depending on Dg, Hg, jvp_vec.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: gradient sensitivity vector (wrt z), for chain rule\n        Hg: Hessian sensitivity vector (wrt z), for chain rule\n        jvp_vec: right sensitivity vector(s) (wrt p) for Hessian-vector-product\n        optimizations: optional optimizations\n    Returns:\n        Hessian/chain rule Hessian as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nzlen, plen = _prod(z.shape), [_prod(param.shape) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\nif jvp_vec is not None:\nassert Dg is not None\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn)\n#############################################################\n# compute 2nd implicit gradients\nif Dg is not None:\nassert Dg.numel() == zlen\nassert Hg is None or Hg.numel() == zlen**2\nDg_ = Dg.reshape((zlen, 1))\nHg_ = Hg.reshape((zlen, zlen)) if Hg is not None else Hg\n# compute the left hand vector in the VJP\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg_.reshape((zlen, 1)), T=True)\nv = v.detach()\nfn = lambda z, *params: torch.sum(v.reshape(zlen) * k_fn(z, *params).reshape(zlen))\nif jvp_vec is not None:\nfor param in params:\nparam.requires_grad = True\nz.requires_grad = True\nDpz_jvp = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\njvp_vec=jvp_vec,\noptimizations=optimizations,\n)\n)\nDpz_jvp = [Dpz_jvp.reshape(-1).detach() for Dpz_jvp in Dpz_jvp]\n# compute the 2nd order derivatives consisting of 4 terms\n# term 1 ##############################\n# Dpp1 = HESSIAN_DIAG(lambda *params: fn(z, *params), *params)\ng_ = grad(fn(z, *params), params, create_graph=True)\nDpp1 = [\nfwd_grad(g_, param, grad_inputs=jvp_vec).reshape(plen)\nfor (g_, param, jvp_vec) in zip(g_, params, jvp_vec)\n]\n# term 2 ##############################\n# temp = JACOBIAN(\n#    lambda z: JACOBIAN(\n#        lambda *params: fn(z, *params), *params, create_graph=True\n#    ),\n#    z,\n# )\n# temp = [temp] if len(params) == 1 else temp\n# temp = [\n#    temp.reshape((plen, zlen)) for (temp, plen) in zip(temp, plen)\n# ]\n# Dpp2 = [\n#    (temp @ Dpz).reshape((plen, plen))\n#    for (temp, Dpz, plen) in zip(temp, Dpz, plen)\n# ]\ng_ = grad(fn(z, *params), params, create_graph=True)\nDpp2 = [\nfwd_grad(g_, z, grad_inputs=Dpz_jvp.reshape(z.shape)).reshape(-1)\nfor (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n]\n# term 3 ##############################\n# Dpp3 = [t(Dpp2) for Dpp2 in Dpp2]\ng_ = grad(fn(z, *params), z, create_graph=True)\ng_ = [\nfwd_grad(g_, param, grad_inputs=jvp_vec)\nfor (param, jvp_vec) in zip(params, jvp_vec)\n]\nDpp3 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nDg=g_,\noptimizations=optimizations,\n)\n)[i].reshape(-1)\nfor (i, g_) in enumerate(g_)\n]\n# term 4 ##############################\n# Dzz = HESSIAN(lambda z: fn(z, *params), z).reshape((zlen, zlen))\n# if Hg is not None:\n#    Dpp4 = [t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\n# else:\n#    Dpp4 = [t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\ng_ = grad(fn(z, *params), z, create_graph=True)\ng_ = [fwd_grad(g_, z, grad_inputs=Dpz_jvp.reshape(z.shape)) for Dpz_jvp in Dpz_jvp]\nif Hg is not None:\ng_ = [\ng_.reshape(zlen) + Hg_ @ Dpz_jvp.reshape(zlen)\nfor (g_, Dpz_jvp) in zip(g_, Dpz_jvp)\n]\nDpp4 = [\n_ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\nDg=g_,\noptimizations=optimizations,\n)\n)[i].reshape(plen)\nfor ((i, g_), plen) in zip(enumerate(g_), plen)\n]\nDp = [Dg_.reshape((1, zlen)) @ Dpz_jvp.reshape(zlen) for Dpz_jvp in Dpz_jvp]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape(()) for Dp in Dp]\nDpp_shaped = [Dpp.reshape(param.shape) for (Dpp, param) in zip(Dpp, params)]\nelse:\n# compute the full first order 1st gradients\nDpz = _ensure_list(\nimplicit_jacobian(\nk_fn,\nz,\n*params,\noptimizations=optimizations,\n)\n)\nDpz = [Dpz.reshape((zlen, plen)).detach() for (Dpz, plen) in zip(Dpz, plen)]\n# compute the 2nd order derivatives consisting of 4 terms\nDpp1 = HESSIAN_DIAG(lambda *params: fn(z, *params), params)\nDpp1 = [Dpp1.reshape((plen, plen)) for (Dpp1, plen) in zip(Dpp1, plen)]\n# temp = JACOBIAN(\n#    lambda z: JACOBIAN(\n#        lambda *params: fn(z, *params), params, create_graph=True\n#    ),\n#    z,\n# )\ntemp = JACOBIAN(\nlambda *params: JACOBIAN(lambda z: fn(z, *params), z, create_graph=True),\nparams,\n)\ntemp = [\ntemp.reshape((zlen, plen)).transpose(-1, -2) for (temp, plen) in zip(temp, plen)\n]\nDpp2 = [\n(temp @ Dpz).reshape((plen, plen)) for (temp, Dpz, plen) in zip(temp, Dpz, plen)\n]\nDpp3 = [t(Dpp2) for Dpp2 in Dpp2]\nDzz = HESSIAN(lambda z: fn(z, *params), z).reshape((zlen, zlen))\nif Hg is not None:\nDpp4 = [t(Dpz) @ (Hg_ + Dzz) @ Dpz for Dpz in Dpz]\nelse:\nDpp4 = [t(Dpz) @ Dzz @ Dpz for Dpz in Dpz]\nDp = [Dg_.reshape((1, zlen)) @ Dpz for Dpz in Dpz]\nDpp = [sum(Dpp) for Dpp in zip(Dpp1, Dpp2, Dpp3, Dpp4)]\n# return the results\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nDpp_shaped = [\nDpp.reshape(param.shape + param.shape) for (Dpp, param) in zip(Dpp, params)\n]\nreturn (Dp_shaped[0], Dpp_shaped[0]) if len(params) == 1 else (Dp_shaped, Dpp_shaped)\nelse:\nDpz, optimizations = implicit_jacobian(\nk_fn,\nz,\n*params,\nfull_output=True,\noptimizations=optimizations,\n)\nDpz = _ensure_list(Dpz)\nDpz = [Dpz.reshape(zlen, plen) for (Dpz, plen) in zip(Dpz, plen)]\n# compute derivatives\nif optimizations.get(\"Dzzk\", None) is None:\nHk = HESSIAN_DIAG(k_fn, (z, *params))\nDzzk, Dppk = Hk[0], Hk[1:]\noptimizations[\"Dzzk\"] = Dzzk\nelse:\nDppk = HESSIAN_DIAG(lambda *params: k_fn(z, *params), params)\nDzpk = JACOBIAN(\nlambda *params: JACOBIAN(lambda z: k_fn(z, *params), z, create_graph=True),\nparams,\n)\nDppk = [Dppk.reshape((zlen, plen, plen)) for (Dppk, plen) in zip(Dppk, plen)]\nDzzk = Dzzk.reshape((zlen, zlen, zlen))\nDzpk = [Dzpk.reshape((zlen, zlen, plen)) for (Dzpk, plen) in zip(Dzpk, plen)]\nDpzk = [Dzpk.transpose(-1, -2) for Dzpk in Dzpk]\n# solve the IFT equation\nlhs = [\nDppk\n+ Dpzk @ Dpz[None, ...]\n+ t(Dpz)[None, ...] @ Dzpk\n+ (t(Dpz)[None, ...] @ Dzzk) @ Dpz[None, ...]\nfor (Dpz, Dzpk, Dpzk, Dppk) in zip(Dpz, Dzpk, Dpzk, Dppk)\n]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDppz = [\n-Dzk_solve_fn(z, *params, rhs=lhs.reshape((zlen, plen * plen)), T=False).reshape(\n(zlen, plen, plen)\n)\nfor (lhs, plen) in zip(lhs, plen)\n]\n# return computed values\nDpz_shaped = [Dpz.reshape(z.shape + param.shape) for (Dpz, param) in zip(Dpz, params)]\nDppz_shaped = [\nDppz.reshape(z.shape + param.shape + param.shape) for (Dppz, param) in zip(Dppz, params)\n]\nreturn (Dpz_shaped[0], Dppz_shaped[0]) if len(params) == 1 else (Dpz_shaped, Dppz_shaped)\n</code></pre>"},{"location":"api/sensitivity_torch/sensitivity/implicit_jacobian/","title":"Implicit jacobian","text":"&lt;&lt;&lt; prev<p>sensitivity_torch.implicit_hessian</p>"},{"location":"api/sensitivity_torch/sensitivity/implicit_jacobian/#sensitivity_torch.sensitivity.implicit_jacobian","title":"<code>sensitivity_torch.sensitivity.implicit_jacobian(k_fn, z, *params, Dg=None, jvp_vec=None, matrix_free_inverse=False, full_output=False, optimizations=None)</code>","text":"<p>Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.</p> <p>Parameters:</p> Name Type Description Default <code>k_fn</code> <code>Callable</code> <p>k_fn(z, *params) = 0, lower/inner implicit function</p> required <code>z</code> <code>Tensor</code> <p>the optimal embedding variable value array</p> required <code>*params</code> <code>Tensor</code> <p>the parameters p of the bilevel program</p> <code>()</code> <code>Dg</code> <code>Tensor</code> <p>left sensitivity vector (wrt z), for a VJP</p> <code>None</code> <code>jvp_vec</code> <code>Union[Tensor, Sequence[Tensor]]</code> <p>right sensitivity vector(s) (wrt p) for a JVP</p> <code>None</code> <code>matrix_free_inverse</code> <code>bool</code> <p>whether to use approximate matrix inversion</p> <code>False</code> <code>full_output</code> <code>bool</code> <p>whether to append accumulated optimizations to the output</p> <code>False</code> <code>optimizations</code> <code>Mapping</code> <p>optional optimizations</p> <code>None</code> <p>Returns:</p> Type Description <p>Jacobian/VJP/JVP as specified by arguments</p> Source code in <code>sensitivity_torch/sensitivity.py</code> <pre><code>def implicit_jacobian(\nk_fn: Callable,\nz: Tensor,\n*params: Tensor,\nDg: Tensor = None,\njvp_vec: Union[Tensor, Sequence[Tensor]] = None,\nmatrix_free_inverse: bool = False,\nfull_output: bool = False,\noptimizations: Mapping = None,\n):\n\"\"\"Computes the implicit Jacobian or VJP or JVP depending on Dg, jvp_vec.\n    Args:\n        k_fn: k_fn(z, *params) = 0, lower/inner implicit function\n        z: the optimal embedding variable value array\n        *params: the parameters p of the bilevel program\n        Dg: left sensitivity vector (wrt z), for a VJP\n        jvp_vec: right sensitivity vector(s) (wrt p) for a JVP\n        matrix_free_inverse: whether to use approximate matrix inversion\n        full_output: whether to append accumulated optimizations to the output\n        optimizations: optional optimizations\n    Returns:\n        Jacobian/VJP/JVP as specified by arguments\n    \"\"\"\noptimizations = {} if optimizations is None else copy(optimizations)\nzlen, plen = _prod(z.shape), [_prod(param.shape) for param in params]\njvp_vec = _ensure_list(jvp_vec) if jvp_vec is not None else jvp_vec\n# construct a default Dzk_solve_fn ##########################\nif optimizations.get(\"Dzk_solve_fn\", None) is None:\n_generate_default_Dzk_solve_fn(optimizations, k_fn)\n#############################################################\nif Dg is not None:\nif matrix_free_inverse:\nA_fn = lambda x: JACOBIAN(\nlambda z: torch.sum(k_fn(z, *params).reshape(-1) * x.reshape(-1)),\nz,\n).reshape(x.shape)\nv = -solve_gmres(A_fn, Dg.reshape((zlen, 1)), max_it=300)\nelse:\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nv = -Dzk_solve_fn(z, *params, rhs=Dg.reshape((zlen, 1)), T=True)\nv = v.detach()\nfn = lambda *params: torch.sum(v.reshape(zlen) * k_fn(z, *params).reshape(zlen))\nDp = JACOBIAN(fn, params)\nDp_shaped = [Dp.reshape(param.shape) for (Dp, param) in zip(Dp, params)]\nret = Dp_shaped[0] if len(params) == 1 else Dp_shaped\nelse:\nif jvp_vec is not None:\nfor param in params:\nparam.requires_grad = True\nf_ = k_fn(z.detach(), *params)\nDp = [\nfwd_grad(f_, param, grad_inputs=jvp_vec)\nfor (param, jvp_vec) in zip(params, jvp_vec)\n]\nDp = [Dp.reshape((zlen, 1)) for (Dp, plen) in zip(Dp, plen)]\nDpk = Dp\nelse:\nDpk = JACOBIAN(lambda *params: k_fn(z, *params), params)\nDpk = [Dpk.reshape((zlen, plen)) for (Dpk, plen) in zip(Dpk, plen)]\nDzk_solve_fn = optimizations[\"Dzk_solve_fn\"]\nDpz = [-Dzk_solve_fn(z, *params, rhs=Dpk, T=False) for Dpk in Dpk]\nif jvp_vec is not None:\nDpz_shaped = [Dpz.reshape(z.shape) for Dpz in Dpz]\nelse:\nDpz_shaped = [Dpz.reshape(z.shape + param.shape) for (Dpz, param) in zip(Dpz, params)]\nret = Dpz_shaped if len(params) != 1 else Dpz_shaped[0]\nreturn (ret, optimizations) if full_output else ret\n</code></pre>"}]}